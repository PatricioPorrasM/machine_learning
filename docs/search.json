[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PRIMER REPORTE EN QUARTO",
    "section": "",
    "text": "Elemento 0\nElemento 1\nElemento 2\nElemento 3\nElemento 4\n\n\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 406 entries, 0 to 405\nData columns (total 9 columns):\n #   Column            Non-Null Count  Dtype         \n---  ------            --------------  -----         \n 0   Name              406 non-null    object        \n 1   Miles_per_Gallon  398 non-null    float64       \n 2   Cylinders         406 non-null    int64         \n 3   Displacement      406 non-null    float64       \n 4   Horsepower        400 non-null    float64       \n 5   Weight_in_lbs     406 non-null    int64         \n 6   Acceleration      406 non-null    float64       \n 7   Year              406 non-null    datetime64[ns]\n 8   Origin            406 non-null    object        \ndtypes: datetime64[ns](1), float64(4), int64(2), object(2)\nmemory usage: 28.7+ KB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Äòcount()‚Äô\n\n\n\n\n\n\n\n\n‚Äòmean‚Äô"
  },
  {
    "objectID": "index.html#agregaciones",
    "href": "index.html#agregaciones",
    "title": "PRIMER REPORTE EN QUARTO",
    "section": "",
    "text": "‚Äòcount()‚Äô\n\n\n\n\n\n\n\n\n‚Äòmean‚Äô"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is one of the fundamental steps in any data science process. It allows us to understand the structure, detect anomalies, and uncover patterns in the data before modeling.\n\n‚ÄúWithout EDA, you‚Äôre not doing data science, you‚Äôre just guessing.‚Äù\n\nEDA combines statistics, programming, and visualization to explore datasets. This report is designed to help you practice these core skills using real-world data.\n\n\nWe will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet‚Äôs load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let‚Äôs examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset.\n\n\n\nBefore diving deeper into the data, it‚Äôs useful to explore some key metadata:\n\n‚úÖ The column names and their data types\n‚ö†Ô∏è The presence of missing values\nüìä Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we‚Äôre dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.3+ KB\n\n\n\n\n\n\nDetecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet‚Äôs start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows √ó 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let‚Äôs set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis.\n\n\n\n\nFinally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows √ó 15 columns"
  },
  {
    "objectID": "eda.html#dataset",
    "href": "eda.html#dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "We will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet‚Äôs load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let‚Äôs examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset."
  },
  {
    "objectID": "eda.html#first-steps",
    "href": "eda.html#first-steps",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Before diving deeper into the data, it‚Äôs useful to explore some key metadata:\n\n‚úÖ The column names and their data types\n‚ö†Ô∏è The presence of missing values\nüìä Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we‚Äôre dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.3+ KB"
  },
  {
    "objectID": "eda.html#missing-values",
    "href": "eda.html#missing-values",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Detecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet‚Äôs start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows √ó 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let‚Äôs set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis."
  },
  {
    "objectID": "eda.html#cleaned-dataset",
    "href": "eda.html#cleaned-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Finally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows √ó 15 columns"
  },
  {
    "objectID": "eda.html#univariate-analysis-quantitative",
    "href": "eda.html#univariate-analysis-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.1 Univariate Analysis: Quantitative",
    "text": "2.1 Univariate Analysis: Quantitative\nA univariate analysis focuses on examining a single numeric variable to understand its distribution, shape, central tendency, and spread. One of the most common tools for this is the histogram.\nIn this case, we‚Äôll explore the distribution of the movie runtime (Running_Time_min).\n\n2.1.1 Basic Histogram\nWe start by creating a histogram to visualize the distribution of running times:\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    title='Histogram of Movie Runtimes (30 bins)'\n)\n\n\n\n\n\n\n\n\nThis chart shows how many movies fall into each time interval (bin). However, histograms can look quite different depending on the number and size of bins used.\n\n\n2.1.2 Effect of Bin Size\nLet‚Äôs compare how the histogram shape changes with different bin sizes:\n\n\nCode\nhistogram_1 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=8)),\n    alt.Y('count()')\n)\n\nhistogram_2 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=10)),\n    alt.Y('count()')\n)\n\nhistogram_1 | histogram_2\n\n\n\n\n\n\n\n\nEven though both plots use the same data, the choice of bin size changes the visual interpretation. A small number of bins may hide details, while too many bins can make it harder to spot trends.\n\n\n2.1.3 Density plots, or Kernel Density Estimate (KDE)\nDensity plots offer a smoothed alternative to histograms. Instead of using rectangular bins to count data points, they estimate the probability density function by placing bell-shaped curves (kernels) at each observation and summing them.\nThis approach helps reduce the visual noise and jaggedness that can occur in histograms and gives a clearer picture of the underlying distribution.\n\n\nCode\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    as_=['Running_Time_min','density'],\n).mark_area().encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q')\n).properties(\n    title=\"Movies runtime\"\n)\n\n\n\n\n\n\n\n\n\n\n2.1.4 Grouped Density plot\nWe can also compare distributions across groups by splitting the KDE by a categorical variable using the groupby parameter. This helps us see how the distribution differs between categories, such as genres.\n\n\nCode\nselection = alt.selection_point(fields=['Major_Genre'], bind='legend')\n\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    groupby=['Major_Genre'],\n    as_=['Running_Time_min', 'density'],\n).mark_area(opacity=0.5).encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q', stack=None),\n    alt.Color('Major_Genre'),\n    opacity=alt.condition(selection, \n        alt.value(1), \n        alt.value(0.05)\n    )\n).add_params(\n    selection\n).properties(\n    title=\"Movies Runtime by Genre (Interactive Filter)\"\n).interactive()\n\n\n\n\n\n\n\n\nThe transparency (opacity=0.5) allows us to observe overlapping distributions and ensures that small density areas are not completely hidden behind larger ones.\nFrom this plot, we can observe, for example, that Drama movies have runtimes nearly as long as the longest Adventure movies, even though their overall distributions differ."
  },
  {
    "objectID": "eda.html#bivariate-analysis-categorical-vs-quantitative",
    "href": "eda.html#bivariate-analysis-categorical-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.2 Bivariate Analysis: Categorical vs Quantitative",
    "text": "2.2 Bivariate Analysis: Categorical vs Quantitative\nBivariate analysis examines the relationship between two variables. In this case, we focus on one categorical variable (e.g., genre) and one quantitative variable (e.g., revenue), which is a very common scenario in exploratory data analysis.\nThis type of analysis is useful to: - Compare average or median values across categories. - Detect outliers or high-variance groups. - Understand distributional differences across categories.\nBelow are several effective visualizations for this analysis.\n\n2.2.1 Basic Barchart\nBar charts are effective for comparing aggregated values (like the mean) across different groups. However, they hide the distribution and variation within each group.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Average Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\nThis bar chart shows the mean Worldwide Gross per genre. It is useful for identifying which genres are more profitable on average, but does not show how spread out the data is.\n\n\n2.2.2 Tick Plot\nTo visualize individual data points, we use a tick plot. This helps uncover variability within genres and detect outliers.\n\n\nCode\nalt.Chart(movies_cleaned).mark_tick().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\"),\n    alt.Tooltip('Title:N')\n).properties(\n    title=\"Individual Gross per Movie by Genre\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.3 Heatmaps\nHeatmaps can summarize the frequency of data points across both axes (quantitative and categorical) using color intensity. It‚Äôs particularly useful for spotting patterns without getting overwhelmed by individual points.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Worldwide_Gross',bin=alt.Bin(maxbins=100)),\n    alt.Y(\"Major_Genre\"),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Heatmap of Movie Counts by Gross and Genre\"\n)\n\n\n\n\n\n\n\n\nThis heatmap shows how frequently movies from each genre fall into different revenue ranges.\n\n\n2.2.4 Boxplot\nBoxplots are useful for comparing distributions across categories and identifying outliers. Boxplots summarize a distribution using five statistics:\n\nMedian (Q2)\nFirst Quartile (Q1)\nThird Quartile (Q3)\nLower Whisker (Q1 - 1.5 √ó IQR)\nUpper Whisker (Q3 + 1.5 √ó IQR)\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Boxplot of Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.5 Side-by-side: Boxplot and Bar Chart\nTo contrast aggregated values (bar chart) with the full distribution (boxplot), we can display them together:\n\n\nCode\nbar = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox = alt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox | bar\n\n\n\n\n\n\n\n\nThis comparison reveals whether the mean is a good representative of the genre, or whether the data is skewed or contains outliers that affect the average"
  },
  {
    "objectID": "eda.html#bivariate-analysis-quantitative-vs-quantitative",
    "href": "eda.html#bivariate-analysis-quantitative-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.3 Bivariate Analysis: Quantitative vs Quantitative",
    "text": "2.3 Bivariate Analysis: Quantitative vs Quantitative\nWhen analyzing two quantitative (numerical) variables simultaneously, we aim to discover possible relationships, trends, or correlations. This type of bivariate analysis can reveal whether increases in one variable are associated with increases or decreases in another (positive or negative correlation), or if there‚Äôs no relationship at all. The most common and intuitive visualization for this is the scatterplot.\n\n2.3.1 Scatterplots\nScatter plots are effective visualizations for exploring two-dimensional distributions, allowing us to identify patterns, trends, clusters, or outliers.\nLet‚Äôs start by visualizing how movies are rated across two popular online platforms:\n\nIMDb\n\nRotten Tomatoes\n\nAre movies rated similarly on different platforms?\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('IMDB_Rating'),\n    alt.Y('Rotten_Tomatoes_Rating')\n).properties(\n    title=\"IMDB vs Rotten Tomatoes Ratings\"\n)\n\n\n\n\n\n\n\n\n\n\n2.3.2 Scatterplot Saturation\nScatterplots can become saturated when too many points overlap in a small area of the chart, making it difficult to distinguish dense regions from sparse ones. For example, when plotting financial variables like production budget versus worldwide gross:\n\n\nCode\nsaturated = alt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('Production_Budget'),\n    alt.Y('Worldwide_Gross')\n).properties(\n    title=\"Saturated Scatterplot: Budget vs Gross\"\n)\nsaturated\n\n\n\n\n\n\n\n\n\n\n2.3.3 Using Binned Heatmap to Reduce Saturation\nTo address saturation, we can bin both variables and use a heatmap where the color intensity represents the number of movies that fall into each rectangular region of the grid. This makes dense areas more interpretable\n\n\nCode\nheatmap_scatter = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Production_Budget', bin=alt.Bin(maxbins=60)),\n    alt.Y('Worldwide_Gross', bin=alt.Bin(maxbins=60)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Binned Heatmap: Budget vs Gross\"\n)\nheatmap_scatter\n\n\n\n\n\n\n\n\n\n\n2.3.4 Side-by-side Comparison\nCompare the raw scatterplot with the heatmap representation:\n\n\nCode\nsaturated | heatmap_scatter"
  },
  {
    "objectID": "eda.html#bivariate-analysis-categorical-vs-categorical",
    "href": "eda.html#bivariate-analysis-categorical-vs-categorical",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.4 Bivariate Analysis: Categorical vs Categorical",
    "text": "2.4 Bivariate Analysis: Categorical vs Categorical\nWhen working with two categorical variables, bivariate analysis helps us understand how categories from one variable relate or are distributed across the other. For example, we might want to know how different movie genres are rated according to the MPAA rating system. Visualization techniques like grouped bar charts and faceted plots can reveal patterns, associations, or class imbalances.\n\n2.4.1 Basic Faceted Bar Chart\nWe begin by exploring how movies are rated (MPAA_Rating) across different genres (Major_Genre). A faceted bar chart allows us to visualize this relationship by plotting a bar chart per genre, helping to identify genre-specific rating distributions.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre'\n)\n\n\n\n\n\n\n\n\n\n\n2.4.2 Vertical Faceting for Alignment\nFaceting horizontally can make comparisons across genres harder when the x-axis is misaligned. By specifying columns=1, we lay out the facets vertically, making it easier to compare counts across genres.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=1\n)\n\n\n\n\n\n\n\n\n\n\n2.4.3 Dependent vs Independent Axis Scaling\nBy default, facet plots share the same x-axis scale (dependent scale), which allows for easier comparison across panels. However, when the number of observations varies greatly between genres, this shared scale can compress some charts.\nWe can instead use independent x-axis scaling for each facet. This highlights the relative distribution within each genre.\n\n\nCode\nshared_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n)\n\nindependent_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n).resolve_scale(x='independent')\n\nshared_scale | independent_scale\n\n\n\n\n\n\n\n\nThe left panel (shared scale) makes absolute comparisons between genres, while the right panel (independent scale) makes within-genre comparisons more readable.\n\n\n2.4.4 Heatmaps\nHeatmaps are effective for visualizing the relationship between two categorical variables when the goal is to display counts or frequency of occurrences. They map the number of observations to color, providing an intuitive view of which category pairs are most or least common.\nWe can enhance this basic representation by also using marker size, combining both color intensity and circle area to represent counts more effectively. This dual encoding can improve interpretation, especially when printed in grayscale or when there are subtle color differences.\n\n\nCode\nheatmap_color = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()')\n).properties(\n    title=\"Heatmap with Color (Count of Movies)\"\n)\n\nheatmap_size = alt.Chart(movies_cleaned).mark_circle().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()'),\n    alt.Size('count()')\n).properties(\n    title=\"Heatmap with Color + Size (Count of Movies)\"\n)\n\nheatmap_color | heatmap_size"
  },
  {
    "objectID": "eda.html#multivariate-analysis",
    "href": "eda.html#multivariate-analysis",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.5 Multivariate Analysis",
    "text": "2.5 Multivariate Analysis\nMultivariate analysis helps us understand the interactions and relationships among multiple variables simultaneously. In the context of numerical features, it is useful to explore pairwise distributions, correlations, and detect potential clusters or anomalies.\nWhen the number of variables is large, repeated charts such as histograms or scatter plot matrices help us summarize patterns efficiently and consistently across all numerical dimensions.\n\n2.5.1 Repeated Histograms for Numerical Columns\nWe first identify and isolate all numerical columns from the dataset. Then we repeat a histogram for each of these columns to understand the individual distributions. This overview is helpful to detect skewness, outliers, or binning decisions that affect how data is grouped visually.\n\n\nCode\n# Select only numerical columns\nnumerical_columns = movies_cleaned.select_dtypes('number').columns.tolist()\n\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X(alt.repeat(),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    width=150,\n    height=150\n).repeat(\n    numerical_columns,\n    columns=4\n)\n\n\n\n\n\n\n\n\n\n\n2.5.2 Scatter Plot Matrix (Pairplot)\nA scatter plot matrix shows the pairwise relationships between all numerical variables. This is a common exploratory tool to detect:\n\nCorrelations between variables\nOutliers or clusters\nRelationships useful for prediction models (e.g., to predict rating or budget)\n\nWe focus especially on the plots below the diagonal, as they are not duplicated.\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='quantitative'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n)\n\n\n\n\n\n\n\n\n\n\n2.5.3 Heatmap Matrix\nWhen scatter plots become too saturated (many overlapping points), heatmaps offer a better alternative by binning the numeric values and encoding the count in color intensity.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X(alt.repeat('column'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y(alt.repeat('row'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n).resolve_scale(\n    color='independent'\n)\n\n\n\n\n\n\n\n\nTo gain deeper insights into the dataset, it‚Äôs important to analyze how numerical variables behave across different categories. This type of multivariate analysis allows us to:\n\nCompare distributions across categories\nDetect outliers within categories\nObserve central tendency (median, quartiles) and spread (range, IQR)\n\nBoxplots are particularly effective for this purpose. In the following visualizations, we explore these relationships by repeating plots across combinations of categorical and numerical features.\n\n\n2.5.4 Filter Categorical Columns\nFirst, we select the relevant categorical columns, excluding identifiers and text-heavy variables like movie titles or director names.\n\n\nCode\ncategorical_columns =  movies_cleaned.select_dtypes('object').columns.to_list()\n\ncategorical_columns_remove = ['Title','Release_Date','Distributor','Director']\n\ncategorical_filtered = [col for col in categorical_columns if col not in categorical_columns_remove]\n\n\n\n\n2.5.5 Repeated Boxplots: Categorical vs Numerical\nWe repeat boxplots using combinations of categorical (rows) and numerical (columns) features. This matrix layout gives a clear visual overview of how numerical values are distributed within each category.\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=numerical_columns,\n    row=categorical_filtered\n)\n\n\n\n\n\n\n\n\n\n\n2.5.6 Faceted Boxplots\nFor more focused analysis, we can facet the boxplots using a specific categorical variable like MPAA_Rating, and repeat the chart by different categorical rows. This lets us keep the numerical axis fixed while comparing how categories vary across different classes (e.g., movie ratings).\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Running_Time_min', type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).facet(\n    column='MPAA_Rating'\n).repeat(\n    row=categorical_filtered\n)"
  },
  {
    "objectID": "laboratorio1.html",
    "href": "laboratorio1.html",
    "title": "Laboratorio 1",
    "section": "",
    "text": "Datos Hist√≥ricos del a√±o 2014 al 2024 referente a la Operaci√≥n Estad√≠stica de Homicidios Intencionales. Datos del mnisterio del interior.\n\n\n\nEvitar restricci√≥n de los 5k registros en el DataSet\n\n\nDataTransformerRegistry.enable('default')\n\n\n\n\nC:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_4768\\348868438.py:1: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n  data_df = pd.read_csv(\"mdi_homicidios_intencionales_pm_2014_2024-completo.csv\")\n\n\n\n\n\n\n\n\n\n\n\ntipo_muerte\nzona\nsubzona\ndistrito\ncircuito\ncodigo_subcircuito\nsubcircuito\ncodigo_provincia\nprovincia\ncodigo_canton\n...\nmedida_edad\nsexo\ngenero\netnia\nestado_civil\nnacionalidad\ndiscapacidad\nprofesion_registro_civil\ninstruccion\nantecedentes\n\n\n\n\n0\nASESINATO\nZONA 1\nESMERALDAS\nESMERALDAS\nLAS PALMAS\n08D01C02S01\nLAS PALMAS 1\n8\nESMERALDAS\n801\n...\nA\nMUJER\nFEMENINO\nAFRO\nSOLTERO\nECUADOR\nNINGUNA\nQUEHACER. DOMESTICOS\nSIN_DATO\nSIN_DATO\n\n\n1\nASESINATO\nZONA 4\nMANAB√ç\nPORTOVIEJO\nSAN PABLO\n13D01C05S02\nSAN PABLO 2\n13\nMANAB√ç\n1301\n...\nA\nHOMBRE\nMASCULINO\nMESTIZO/A\nSOLTERO\nECUADOR\nNINGUNA\nEMPLEADO PARTICULAR\nBASICA\nSIN_DATO\n\n\n2\nASESINATO\nZONA 4\nMANAB√ç\nPORTOVIEJO\nSAN PABLO\n13D01C05S02\nSAN PABLO 2\n13\nMANAB√ç\n1301\n...\nA\nHOMBRE\nMASCULINO\nMESTIZO/A\nSOLTERO\nECUADOR\nNINGUNA\nALBANIL\nSIN_DATO\nSIN_DATO\n\n\n3\nASESINATO\nZONA 4\nMANAB√ç\nMANTA\nLA PILA\n13D02C14S01\nLA PILA 1\n13\nMANAB√ç\n1309\n...\nA\nMUJER\nFEMENINO\nMESTIZO/A\nSOLTERO\nECUADOR\nNINGUNA\nNINGUNA\nSIN_DATO\nSIN_DATO\n\n\n4\nASESINATO\nZONA 4\nMANAB√ç\nMANTA\nLA PILA\n13D02C14S01\nLA PILA 1\n13\nMANAB√ç\n1309\n...\nA\nMUJER\nFEMENINO\nMESTIZO/A\nSOLTERO\nECUADOR\nNINGUNA\nQUEHACER. DOMESTICOS\nSECUNDARIA\nSIN_DATO\n\n\n\n\n5 rows √ó 34 columns\n\n\n\n\n\n\n\n\n\n\n\n\ncodigo_provincia\ncodigo_canton\n\n\n\n\ncount\n30538.000000\n30538.000000\n\n\nmean\n10.942891\n1098.638090\n\n\nstd\n4.614796\n460.944665\n\n\nmin\n1.000000\n101.000000\n\n\n25%\n9.000000\n901.000000\n\n\n50%\n9.000000\n907.000000\n\n\n75%\n13.000000\n1301.000000\n\n\nmax\n24.000000\n2403.000000\n\n\n\n\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30538 entries, 0 to 30537\nData columns (total 34 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   tipo_muerte               30538 non-null  object\n 1   zona                      30538 non-null  object\n 2   subzona                   30538 non-null  object\n 3   distrito                  30538 non-null  object\n 4   circuito                  30538 non-null  object\n 5   codigo_subcircuito        30538 non-null  object\n 6   subcircuito               30538 non-null  object\n 7   codigo_provincia          30538 non-null  int64 \n 8   provincia                 30538 non-null  object\n 9   codigo_canton             30538 non-null  int64 \n 10  canton                    30538 non-null  object\n 11  coordenada_y              30538 non-null  object\n 12  coordenada_x              30538 non-null  object\n 13  area_hecho                30538 non-null  object\n 14  lugar                     30538 non-null  object\n 15  tipo_lugar                30538 non-null  object\n 16  fecha_infraccion          30538 non-null  object\n 17  hora_infraccion           30538 non-null  object\n 18  arma                      30538 non-null  object\n 19  tipo_arma                 30538 non-null  object\n 20  presunta_motivacion       30538 non-null  object\n 21  presun_motiva_observada   30538 non-null  object\n 22  probable_causa_motivada   30538 non-null  object\n 23  edad                      30538 non-null  object\n 24  medida_edad               30538 non-null  object\n 25  sexo                      30538 non-null  object\n 26  genero                    30538 non-null  object\n 27  etnia                     30538 non-null  object\n 28  estado_civil              30538 non-null  object\n 29  nacionalidad              30538 non-null  object\n 30  discapacidad              30538 non-null  object\n 31  profesion_registro_civil  30538 non-null  object\n 32  instruccion               30538 non-null  object\n 33  antecedentes              30538 non-null  object\ndtypes: int64(2), object(32)\nmemory usage: 7.9+ MB\n\n\n\n\nC:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_4768\\3683265002.py:1: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n  data_df[\"fecha_infraccion_dt\"] = pd.to_datetime(data_df[\"fecha_infraccion\"], errors=\"coerce\")\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30538 entries, 0 to 30537\nData columns (total 36 columns):\n #   Column                    Non-Null Count  Dtype         \n---  ------                    --------------  -----         \n 0   tipo_muerte               30538 non-null  object        \n 1   zona                      30538 non-null  object        \n 2   subzona                   30538 non-null  object        \n 3   distrito                  30538 non-null  object        \n 4   circuito                  30538 non-null  object        \n 5   codigo_subcircuito        30538 non-null  object        \n 6   subcircuito               30538 non-null  object        \n 7   codigo_provincia          30538 non-null  int64         \n 8   provincia                 30538 non-null  object        \n 9   codigo_canton             30538 non-null  int64         \n 10  canton                    30538 non-null  object        \n 11  coordenada_y              30538 non-null  object        \n 12  coordenada_x              30538 non-null  object        \n 13  area_hecho                30538 non-null  object        \n 14  lugar                     30538 non-null  object        \n 15  tipo_lugar                30538 non-null  object        \n 16  fecha_infraccion          30538 non-null  object        \n 17  hora_infraccion           30538 non-null  object        \n 18  arma                      30538 non-null  object        \n 19  tipo_arma                 30538 non-null  object        \n 20  presunta_motivacion       30538 non-null  object        \n 21  presun_motiva_observada   30538 non-null  object        \n 22  probable_causa_motivada   30538 non-null  object        \n 23  edad                      30538 non-null  float64       \n 24  medida_edad               30538 non-null  object        \n 25  sexo                      30538 non-null  object        \n 26  genero                    30538 non-null  object        \n 27  etnia                     30538 non-null  object        \n 28  estado_civil              30538 non-null  object        \n 29  nacionalidad              30538 non-null  object        \n 30  discapacidad              30538 non-null  object        \n 31  profesion_registro_civil  30538 non-null  object        \n 32  instruccion               30538 non-null  object        \n 33  antecedentes              30538 non-null  object        \n 34  fecha_infraccion_dt       30538 non-null  datetime64[ns]\n 35  year                      30538 non-null  int32         \ndtypes: datetime64[ns](1), float64(1), int32(1), int64(2), object(31)\nmemory usage: 8.3+ MB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#{python} alt.Chart(data_df).mark_bar().encode(     x=alt.X('year:O', title='A√±o'),     y=alt.Y('count():Q', title='Total de homicidios'),     tooltip=['year', 'count()'] ).properties(     title='Tendencia de homicidios por a√±o' ) #\n#{python} alt.Chart(data_df).mark_bar().encode(     x=alt.X('provincia:N', sort='-y', title='Provincia', axis=alt.Axis(labelAngle=-45)),     y=alt.Y('count():Q', title='Total de homicidios'),     tooltip=['provincia', 'count()'] ).properties(     title='Total de homicidios por provincia' ) #\n#{python} alt.Chart(data_df).mark_bar().encode(     x=alt.X('edad:Q', bin=alt.Bin(maxbins=30), title='Edad de la v√≠ctima'),     y=alt.Y('count():Q', title='N√∫mero de v√≠ctimas'),     tooltip=['edad', 'count()'] ).properties(     title='Distribuci√≥n de edades de las v√≠ctimas' ) #\n#{python} alt.Chart(data_df).mark_bar().encode(     x=alt.X('arma:N', sort='-y', title='Tipo de arma'),     y=alt.Y('count():Q', title='Total de homicidios'),     color=alt.Color('arma:N', legend=None),     tooltip=['arma', 'count()'] ).properties(     title='Total de homicidios por tipo de arma' ) #\n#```{python} data_df[‚Äúmes‚Äù] = data_df[‚Äúfecha_infraccion_dt‚Äù].dt.month\nalt.Chart(data_df).mark_line(point=True).encode( x=alt.X(‚Äòmes:O‚Äô, title=‚ÄòMes‚Äô), y=alt.Y(‚Äòcount():Q‚Äô, title=‚ÄòTotal de homicidios‚Äô), color=alt.Color(‚Äòyear:N‚Äô, title=‚ÄòA√±o‚Äô), tooltip=[‚Äòyear‚Äô, ‚Äòmes‚Äô, ‚Äòcount()‚Äô] ).properties( title=‚ÄòHomicidios por mes y a√±o‚Äô ) #```\n#```{python} df_edad_prov = data_df.groupby(‚Äòprovincia‚Äô)[‚Äòedad‚Äô].mean().reset_index()\nalt.Chart(df_edad_prov).mark_bar().encode( x=alt.X(‚Äòprovincia:N‚Äô, sort=‚Äò-y‚Äô, title=‚ÄòProvincia‚Äô, axis=alt.Axis(labelAngle=-45)), y=alt.Y(‚Äòedad:Q‚Äô, title=‚ÄòEdad promedio‚Äô), tooltip=[‚Äòprovincia‚Äô, ‚Äòedad‚Äô] ).properties( title=‚ÄòEdad promedio de v√≠ctimas por provincia‚Äô ) #```\n#```{python} df_grouped = data_df.groupby([‚Äòprovincia‚Äô, ‚Äòarma‚Äô]).size().reset_index(name=‚Äòtotal_muertes‚Äô)\nalt.Chart(df_grouped).mark_point(filled=True, size=100, opacity=0.8).encode( x=alt.X(‚Äúprovincia:N‚Äù, title=‚ÄúProvincia‚Äù, sort=‚Äú-y‚Äù, axis=alt.Axis(labelAngle=-45)), y=alt.Y(‚Äúarma:N‚Äù, title=‚ÄúTipo de arma‚Äù), size=alt.Size(‚Äútotal_muertes:Q‚Äù, title=‚ÄúTotal de muertes‚Äù), color=alt.Color(‚Äúarma:N‚Äù, title=‚ÄúTipo de arma‚Äù), tooltip=[‚Äúprovincia‚Äù, ‚Äúarma‚Äù, ‚Äútotal_muertes‚Äù] ).properties( title=‚ÄúTotal de muertes por tipo de arma y provincia‚Äù, width=700, height=400 ).interactive() #```"
  },
  {
    "objectID": "laboratorio1.html#contexto",
    "href": "laboratorio1.html#contexto",
    "title": "Laboratorio 1",
    "section": "",
    "text": "Datos Hist√≥ricos del a√±o 2014 al 2024 referente a la Operaci√≥n Estad√≠stica de Homicidios Intencionales. Datos del mnisterio del interior."
  },
  {
    "objectID": "laboratorio1.html#carga-de-datos",
    "href": "laboratorio1.html#carga-de-datos",
    "title": "Laboratorio 1",
    "section": "",
    "text": "Evitar restricci√≥n de los 5k registros en el DataSet\n\n\nDataTransformerRegistry.enable('default')\n\n\n\n\nC:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_4768\\348868438.py:1: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n  data_df = pd.read_csv(\"mdi_homicidios_intencionales_pm_2014_2024-completo.csv\")\n\n\n\n\n\n\n\n\n\n\n\ntipo_muerte\nzona\nsubzona\ndistrito\ncircuito\ncodigo_subcircuito\nsubcircuito\ncodigo_provincia\nprovincia\ncodigo_canton\n...\nmedida_edad\nsexo\ngenero\netnia\nestado_civil\nnacionalidad\ndiscapacidad\nprofesion_registro_civil\ninstruccion\nantecedentes\n\n\n\n\n0\nASESINATO\nZONA 1\nESMERALDAS\nESMERALDAS\nLAS PALMAS\n08D01C02S01\nLAS PALMAS 1\n8\nESMERALDAS\n801\n...\nA\nMUJER\nFEMENINO\nAFRO\nSOLTERO\nECUADOR\nNINGUNA\nQUEHACER. DOMESTICOS\nSIN_DATO\nSIN_DATO\n\n\n1\nASESINATO\nZONA 4\nMANAB√ç\nPORTOVIEJO\nSAN PABLO\n13D01C05S02\nSAN PABLO 2\n13\nMANAB√ç\n1301\n...\nA\nHOMBRE\nMASCULINO\nMESTIZO/A\nSOLTERO\nECUADOR\nNINGUNA\nEMPLEADO PARTICULAR\nBASICA\nSIN_DATO\n\n\n2\nASESINATO\nZONA 4\nMANAB√ç\nPORTOVIEJO\nSAN PABLO\n13D01C05S02\nSAN PABLO 2\n13\nMANAB√ç\n1301\n...\nA\nHOMBRE\nMASCULINO\nMESTIZO/A\nSOLTERO\nECUADOR\nNINGUNA\nALBANIL\nSIN_DATO\nSIN_DATO\n\n\n3\nASESINATO\nZONA 4\nMANAB√ç\nMANTA\nLA PILA\n13D02C14S01\nLA PILA 1\n13\nMANAB√ç\n1309\n...\nA\nMUJER\nFEMENINO\nMESTIZO/A\nSOLTERO\nECUADOR\nNINGUNA\nNINGUNA\nSIN_DATO\nSIN_DATO\n\n\n4\nASESINATO\nZONA 4\nMANAB√ç\nMANTA\nLA PILA\n13D02C14S01\nLA PILA 1\n13\nMANAB√ç\n1309\n...\nA\nMUJER\nFEMENINO\nMESTIZO/A\nSOLTERO\nECUADOR\nNINGUNA\nQUEHACER. DOMESTICOS\nSECUNDARIA\nSIN_DATO\n\n\n\n\n5 rows √ó 34 columns\n\n\n\n\n\n\n\n\n\n\n\n\ncodigo_provincia\ncodigo_canton\n\n\n\n\ncount\n30538.000000\n30538.000000\n\n\nmean\n10.942891\n1098.638090\n\n\nstd\n4.614796\n460.944665\n\n\nmin\n1.000000\n101.000000\n\n\n25%\n9.000000\n901.000000\n\n\n50%\n9.000000\n907.000000\n\n\n75%\n13.000000\n1301.000000\n\n\nmax\n24.000000\n2403.000000\n\n\n\n\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30538 entries, 0 to 30537\nData columns (total 34 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   tipo_muerte               30538 non-null  object\n 1   zona                      30538 non-null  object\n 2   subzona                   30538 non-null  object\n 3   distrito                  30538 non-null  object\n 4   circuito                  30538 non-null  object\n 5   codigo_subcircuito        30538 non-null  object\n 6   subcircuito               30538 non-null  object\n 7   codigo_provincia          30538 non-null  int64 \n 8   provincia                 30538 non-null  object\n 9   codigo_canton             30538 non-null  int64 \n 10  canton                    30538 non-null  object\n 11  coordenada_y              30538 non-null  object\n 12  coordenada_x              30538 non-null  object\n 13  area_hecho                30538 non-null  object\n 14  lugar                     30538 non-null  object\n 15  tipo_lugar                30538 non-null  object\n 16  fecha_infraccion          30538 non-null  object\n 17  hora_infraccion           30538 non-null  object\n 18  arma                      30538 non-null  object\n 19  tipo_arma                 30538 non-null  object\n 20  presunta_motivacion       30538 non-null  object\n 21  presun_motiva_observada   30538 non-null  object\n 22  probable_causa_motivada   30538 non-null  object\n 23  edad                      30538 non-null  object\n 24  medida_edad               30538 non-null  object\n 25  sexo                      30538 non-null  object\n 26  genero                    30538 non-null  object\n 27  etnia                     30538 non-null  object\n 28  estado_civil              30538 non-null  object\n 29  nacionalidad              30538 non-null  object\n 30  discapacidad              30538 non-null  object\n 31  profesion_registro_civil  30538 non-null  object\n 32  instruccion               30538 non-null  object\n 33  antecedentes              30538 non-null  object\ndtypes: int64(2), object(32)\nmemory usage: 7.9+ MB\n\n\n\n\nC:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_4768\\3683265002.py:1: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n  data_df[\"fecha_infraccion_dt\"] = pd.to_datetime(data_df[\"fecha_infraccion\"], errors=\"coerce\")\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30538 entries, 0 to 30537\nData columns (total 36 columns):\n #   Column                    Non-Null Count  Dtype         \n---  ------                    --------------  -----         \n 0   tipo_muerte               30538 non-null  object        \n 1   zona                      30538 non-null  object        \n 2   subzona                   30538 non-null  object        \n 3   distrito                  30538 non-null  object        \n 4   circuito                  30538 non-null  object        \n 5   codigo_subcircuito        30538 non-null  object        \n 6   subcircuito               30538 non-null  object        \n 7   codigo_provincia          30538 non-null  int64         \n 8   provincia                 30538 non-null  object        \n 9   codigo_canton             30538 non-null  int64         \n 10  canton                    30538 non-null  object        \n 11  coordenada_y              30538 non-null  object        \n 12  coordenada_x              30538 non-null  object        \n 13  area_hecho                30538 non-null  object        \n 14  lugar                     30538 non-null  object        \n 15  tipo_lugar                30538 non-null  object        \n 16  fecha_infraccion          30538 non-null  object        \n 17  hora_infraccion           30538 non-null  object        \n 18  arma                      30538 non-null  object        \n 19  tipo_arma                 30538 non-null  object        \n 20  presunta_motivacion       30538 non-null  object        \n 21  presun_motiva_observada   30538 non-null  object        \n 22  probable_causa_motivada   30538 non-null  object        \n 23  edad                      30538 non-null  float64       \n 24  medida_edad               30538 non-null  object        \n 25  sexo                      30538 non-null  object        \n 26  genero                    30538 non-null  object        \n 27  etnia                     30538 non-null  object        \n 28  estado_civil              30538 non-null  object        \n 29  nacionalidad              30538 non-null  object        \n 30  discapacidad              30538 non-null  object        \n 31  profesion_registro_civil  30538 non-null  object        \n 32  instruccion               30538 non-null  object        \n 33  antecedentes              30538 non-null  object        \n 34  fecha_infraccion_dt       30538 non-null  datetime64[ns]\n 35  year                      30538 non-null  int32         \ndtypes: datetime64[ns](1), float64(1), int32(1), int64(2), object(31)\nmemory usage: 8.3+ MB"
  },
  {
    "objectID": "laboratorio1.html#graficas",
    "href": "laboratorio1.html#graficas",
    "title": "Laboratorio 1",
    "section": "",
    "text": "#{python} alt.Chart(data_df).mark_bar().encode(     x=alt.X('year:O', title='A√±o'),     y=alt.Y('count():Q', title='Total de homicidios'),     tooltip=['year', 'count()'] ).properties(     title='Tendencia de homicidios por a√±o' ) #\n#{python} alt.Chart(data_df).mark_bar().encode(     x=alt.X('provincia:N', sort='-y', title='Provincia', axis=alt.Axis(labelAngle=-45)),     y=alt.Y('count():Q', title='Total de homicidios'),     tooltip=['provincia', 'count()'] ).properties(     title='Total de homicidios por provincia' ) #\n#{python} alt.Chart(data_df).mark_bar().encode(     x=alt.X('edad:Q', bin=alt.Bin(maxbins=30), title='Edad de la v√≠ctima'),     y=alt.Y('count():Q', title='N√∫mero de v√≠ctimas'),     tooltip=['edad', 'count()'] ).properties(     title='Distribuci√≥n de edades de las v√≠ctimas' ) #\n#{python} alt.Chart(data_df).mark_bar().encode(     x=alt.X('arma:N', sort='-y', title='Tipo de arma'),     y=alt.Y('count():Q', title='Total de homicidios'),     color=alt.Color('arma:N', legend=None),     tooltip=['arma', 'count()'] ).properties(     title='Total de homicidios por tipo de arma' ) #\n#```{python} data_df[‚Äúmes‚Äù] = data_df[‚Äúfecha_infraccion_dt‚Äù].dt.month\nalt.Chart(data_df).mark_line(point=True).encode( x=alt.X(‚Äòmes:O‚Äô, title=‚ÄòMes‚Äô), y=alt.Y(‚Äòcount():Q‚Äô, title=‚ÄòTotal de homicidios‚Äô), color=alt.Color(‚Äòyear:N‚Äô, title=‚ÄòA√±o‚Äô), tooltip=[‚Äòyear‚Äô, ‚Äòmes‚Äô, ‚Äòcount()‚Äô] ).properties( title=‚ÄòHomicidios por mes y a√±o‚Äô ) #```\n#```{python} df_edad_prov = data_df.groupby(‚Äòprovincia‚Äô)[‚Äòedad‚Äô].mean().reset_index()\nalt.Chart(df_edad_prov).mark_bar().encode( x=alt.X(‚Äòprovincia:N‚Äô, sort=‚Äò-y‚Äô, title=‚ÄòProvincia‚Äô, axis=alt.Axis(labelAngle=-45)), y=alt.Y(‚Äòedad:Q‚Äô, title=‚ÄòEdad promedio‚Äô), tooltip=[‚Äòprovincia‚Äô, ‚Äòedad‚Äô] ).properties( title=‚ÄòEdad promedio de v√≠ctimas por provincia‚Äô ) #```\n#```{python} df_grouped = data_df.groupby([‚Äòprovincia‚Äô, ‚Äòarma‚Äô]).size().reset_index(name=‚Äòtotal_muertes‚Äô)\nalt.Chart(df_grouped).mark_point(filled=True, size=100, opacity=0.8).encode( x=alt.X(‚Äúprovincia:N‚Äù, title=‚ÄúProvincia‚Äù, sort=‚Äú-y‚Äù, axis=alt.Axis(labelAngle=-45)), y=alt.Y(‚Äúarma:N‚Äù, title=‚ÄúTipo de arma‚Äù), size=alt.Size(‚Äútotal_muertes:Q‚Äù, title=‚ÄúTotal de muertes‚Äù), color=alt.Color(‚Äúarma:N‚Äù, title=‚ÄúTipo de arma‚Äù), tooltip=[‚Äúprovincia‚Äù, ‚Äúarma‚Äù, ‚Äútotal_muertes‚Äù] ).properties( title=‚ÄúTotal de muertes por tipo de arma y provincia‚Äù, width=700, height=400 ).interactive() #```"
  },
  {
    "objectID": "clasification.html",
    "href": "clasification.html",
    "title": "Clasification",
    "section": "",
    "text": "Code\n# 1. Import libraries\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\n\n\n\nCode\ndata = load_breast_cancer()\n\nX = data.data\ny = data.target\n\ndata\n\n\n{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n         1.189e-01],\n        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n         8.902e-02],\n        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n         8.758e-02],\n        ...,\n        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n         7.820e-02],\n        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n         1.240e-01],\n        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n         7.039e-02]], shape=(569, 30)),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n 'frame': None,\n 'target_names': array(['malignant', 'benign'], dtype='&lt;U9'),\n 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer Wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 569\\n\\n:Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n:Attribute Information:\\n    - radius (mean of distances from center to points on the perimeter)\\n    - texture (standard deviation of gray-scale values)\\n    - perimeter\\n    - area\\n    - smoothness (local variation in radius lengths)\\n    - compactness (perimeter^2 / area - 1.0)\\n    - concavity (severity of concave portions of the contour)\\n    - concave points (number of concave portions of the contour)\\n    - symmetry\\n    - fractal dimension (\"coastline approximation\" - 1)\\n\\n    The mean, standard error, and \"worst\" or largest (mean of the three\\n    worst/largest values) of these features were computed for each image,\\n    resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n    10 is Radius SE, field 20 is Worst Radius.\\n\\n    - class:\\n            - WDBC-Malignant\\n            - WDBC-Benign\\n\\n:Summary Statistics:\\n\\n===================================== ====== ======\\n                                        Min    Max\\n===================================== ====== ======\\nradius (mean):                        6.981  28.11\\ntexture (mean):                       9.71   39.28\\nperimeter (mean):                     43.79  188.5\\narea (mean):                          143.5  2501.0\\nsmoothness (mean):                    0.053  0.163\\ncompactness (mean):                   0.019  0.345\\nconcavity (mean):                     0.0    0.427\\nconcave points (mean):                0.0    0.201\\nsymmetry (mean):                      0.106  0.304\\nfractal dimension (mean):             0.05   0.097\\nradius (standard error):              0.112  2.873\\ntexture (standard error):             0.36   4.885\\nperimeter (standard error):           0.757  21.98\\narea (standard error):                6.802  542.2\\nsmoothness (standard error):          0.002  0.031\\ncompactness (standard error):         0.002  0.135\\nconcavity (standard error):           0.0    0.396\\nconcave points (standard error):      0.0    0.053\\nsymmetry (standard error):            0.008  0.079\\nfractal dimension (standard error):   0.001  0.03\\nradius (worst):                       7.93   36.04\\ntexture (worst):                      12.02  49.54\\nperimeter (worst):                    50.41  251.2\\narea (worst):                         185.2  4254.0\\nsmoothness (worst):                   0.071  0.223\\ncompactness (worst):                  0.027  1.058\\nconcavity (worst):                    0.0    1.252\\nconcave points (worst):               0.0    0.291\\nsymmetry (worst):                     0.156  0.664\\nfractal dimension (worst):            0.055  0.208\\n===================================== ====== ======\\n\\n:Missing Attribute Values: None\\n\\n:Class Distribution: 212 - Malignant, 357 - Benign\\n\\n:Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n:Donor: Nick Street\\n\\n:Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. dropdown:: References\\n\\n  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\\n    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\\n    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n    San Jose, CA, 1993.\\n  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\\n    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\\n    July-August 1995.\\n  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\\n    163-171.\\n',\n 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n        'mean smoothness', 'mean compactness', 'mean concavity',\n        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n        'radius error', 'texture error', 'perimeter error', 'area error',\n        'smoothness error', 'compactness error', 'concavity error',\n        'concave points error', 'symmetry error',\n        'fractal dimension error', 'worst radius', 'worst texture',\n        'worst perimeter', 'worst area', 'worst smoothness',\n        'worst compactness', 'worst concavity', 'worst concave points',\n        'worst symmetry', 'worst fractal dimension'], dtype='&lt;U23'),\n 'filename': 'breast_cancer.csv',\n 'data_module': 'sklearn.datasets.data'}\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=40, stratify=y)\n\n# print(\"X_train\")\n# print(X_train)\n# print(\"X_test\")\n# print(X_test)\n# print(\"y_train\")\n# print(y_train)\n# print(\"y_test\")\n# print(y_test)\n\n\n\n\n\n\n\nCode\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n\nC:\\Personal\\Yachay\\machine_learning\\.venv-13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n\nIncrease the number of iterations to improve the convergence (max_iter=1000).\nYou might also want to scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty¬†\n'l2'\n\n\n\ndual¬†\nFalse\n\n\n\ntol¬†\n0.0001\n\n\n\nC¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\nintercept_scaling¬†\n1\n\n\n\nclass_weight¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nsolver¬†\n'lbfgs'\n\n\n\nmax_iter¬†\n1000\n\n\n\nmulti_class¬†\n'deprecated'\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nn_jobs¬†\nNone\n\n\n\nl1_ratio¬†\nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_pred = model.predict(X_test)\n\n\n\n\n\n\n\nCode\naccuray = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\"accuray = {accuray}\")\nprint(f\"precision = {precision}\")\nprint(f\"recall = {recall}\")\nprint(f\"f1 = {f1}\")\n\n\naccuray = 0.9912280701754386\nprecision = 0.9863013698630136\nrecall = 1.0\nf1 = 0.993103448275862\n\n\n\n\n\n\n\nCode\nConfusionMatrixDisplay.from_predictions (y_test, y_pred)"
  },
  {
    "objectID": "clasification.html#importar-librer√≠as",
    "href": "clasification.html#importar-librer√≠as",
    "title": "Clasification",
    "section": "",
    "text": "Code\n# 1. Import libraries\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "clasification.html#cargar-e-dataset",
    "href": "clasification.html#cargar-e-dataset",
    "title": "Clasification",
    "section": "",
    "text": "Code\ndata = load_breast_cancer()\n\nX = data.data\ny = data.target\n\ndata\n\n\n{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n         1.189e-01],\n        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n         8.902e-02],\n        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n         8.758e-02],\n        ...,\n        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n         7.820e-02],\n        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n         1.240e-01],\n        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n         7.039e-02]], shape=(569, 30)),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n 'frame': None,\n 'target_names': array(['malignant', 'benign'], dtype='&lt;U9'),\n 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer Wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 569\\n\\n:Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n:Attribute Information:\\n    - radius (mean of distances from center to points on the perimeter)\\n    - texture (standard deviation of gray-scale values)\\n    - perimeter\\n    - area\\n    - smoothness (local variation in radius lengths)\\n    - compactness (perimeter^2 / area - 1.0)\\n    - concavity (severity of concave portions of the contour)\\n    - concave points (number of concave portions of the contour)\\n    - symmetry\\n    - fractal dimension (\"coastline approximation\" - 1)\\n\\n    The mean, standard error, and \"worst\" or largest (mean of the three\\n    worst/largest values) of these features were computed for each image,\\n    resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n    10 is Radius SE, field 20 is Worst Radius.\\n\\n    - class:\\n            - WDBC-Malignant\\n            - WDBC-Benign\\n\\n:Summary Statistics:\\n\\n===================================== ====== ======\\n                                        Min    Max\\n===================================== ====== ======\\nradius (mean):                        6.981  28.11\\ntexture (mean):                       9.71   39.28\\nperimeter (mean):                     43.79  188.5\\narea (mean):                          143.5  2501.0\\nsmoothness (mean):                    0.053  0.163\\ncompactness (mean):                   0.019  0.345\\nconcavity (mean):                     0.0    0.427\\nconcave points (mean):                0.0    0.201\\nsymmetry (mean):                      0.106  0.304\\nfractal dimension (mean):             0.05   0.097\\nradius (standard error):              0.112  2.873\\ntexture (standard error):             0.36   4.885\\nperimeter (standard error):           0.757  21.98\\narea (standard error):                6.802  542.2\\nsmoothness (standard error):          0.002  0.031\\ncompactness (standard error):         0.002  0.135\\nconcavity (standard error):           0.0    0.396\\nconcave points (standard error):      0.0    0.053\\nsymmetry (standard error):            0.008  0.079\\nfractal dimension (standard error):   0.001  0.03\\nradius (worst):                       7.93   36.04\\ntexture (worst):                      12.02  49.54\\nperimeter (worst):                    50.41  251.2\\narea (worst):                         185.2  4254.0\\nsmoothness (worst):                   0.071  0.223\\ncompactness (worst):                  0.027  1.058\\nconcavity (worst):                    0.0    1.252\\nconcave points (worst):               0.0    0.291\\nsymmetry (worst):                     0.156  0.664\\nfractal dimension (worst):            0.055  0.208\\n===================================== ====== ======\\n\\n:Missing Attribute Values: None\\n\\n:Class Distribution: 212 - Malignant, 357 - Benign\\n\\n:Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n:Donor: Nick Street\\n\\n:Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. dropdown:: References\\n\\n  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\\n    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\\n    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n    San Jose, CA, 1993.\\n  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\\n    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\\n    July-August 1995.\\n  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\\n    163-171.\\n',\n 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n        'mean smoothness', 'mean compactness', 'mean concavity',\n        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n        'radius error', 'texture error', 'perimeter error', 'area error',\n        'smoothness error', 'compactness error', 'concavity error',\n        'concave points error', 'symmetry error',\n        'fractal dimension error', 'worst radius', 'worst texture',\n        'worst perimeter', 'worst area', 'worst smoothness',\n        'worst compactness', 'worst concavity', 'worst concave points',\n        'worst symmetry', 'worst fractal dimension'], dtype='&lt;U23'),\n 'filename': 'breast_cancer.csv',\n 'data_module': 'sklearn.datasets.data'}"
  },
  {
    "objectID": "clasification.html#split",
    "href": "clasification.html#split",
    "title": "Clasification",
    "section": "",
    "text": "Code\nX_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=40, stratify=y)\n\n# print(\"X_train\")\n# print(X_train)\n# print(\"X_test\")\n# print(X_test)\n# print(\"y_train\")\n# print(y_train)\n# print(\"y_test\")\n# print(y_test)"
  },
  {
    "objectID": "clasification.html#entrenar-el-modelo",
    "href": "clasification.html#entrenar-el-modelo",
    "title": "Clasification",
    "section": "",
    "text": "Code\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n\nC:\\Personal\\Yachay\\machine_learning\\.venv-13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n\nIncrease the number of iterations to improve the convergence (max_iter=1000).\nYou might also want to scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty¬†\n'l2'\n\n\n\ndual¬†\nFalse\n\n\n\ntol¬†\n0.0001\n\n\n\nC¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\nintercept_scaling¬†\n1\n\n\n\nclass_weight¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nsolver¬†\n'lbfgs'\n\n\n\nmax_iter¬†\n1000\n\n\n\nmulti_class¬†\n'deprecated'\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nn_jobs¬†\nNone\n\n\n\nl1_ratio¬†\nNone"
  },
  {
    "objectID": "clasification.html#predicci√≥n",
    "href": "clasification.html#predicci√≥n",
    "title": "Clasification",
    "section": "",
    "text": "Code\ny_pred = model.predict(X_test)"
  },
  {
    "objectID": "clasification.html#evaluaci√≥ncls",
    "href": "clasification.html#evaluaci√≥ncls",
    "title": "Clasification",
    "section": "",
    "text": "Code\naccuray = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\"accuray = {accuray}\")\nprint(f\"precision = {precision}\")\nprint(f\"recall = {recall}\")\nprint(f\"f1 = {f1}\")\n\n\naccuray = 0.9276315789473685\nprecision = 0.953405017921147\nrecall = 0.9300699300699301\nf1 = 0.9415929203539823"
  },
  {
    "objectID": "clasification.html#matriz-de-confusi√≥n",
    "href": "clasification.html#matriz-de-confusi√≥n",
    "title": "Clasification",
    "section": "",
    "text": "Code\nConfusionMatrixDisplay.from_predictions (y_test, y_pred)"
  },
  {
    "objectID": "clasification.html#cargar-e-dataset-1",
    "href": "clasification.html#cargar-e-dataset-1",
    "title": "Clasification",
    "section": "2. Cargar e dataset",
    "text": "2. Cargar e dataset\n\n\nCode\ndata = load_breast_cancer()\n\nX = data.data #features\ny = data.target #target"
  },
  {
    "objectID": "clasification.html#split-1",
    "href": "clasification.html#split-1",
    "title": "Clasification",
    "section": "3. Split",
    "text": "3. Split\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=40, stratify=y)"
  },
  {
    "objectID": "clasification.html#construcci√≥n-del-pipeline",
    "href": "clasification.html#construcci√≥n-del-pipeline",
    "title": "Clasification",
    "section": "4. Construcci√≥n del pipeline",
    "text": "4. Construcci√≥n del pipeline\n\n\nCode\nsteps = [(\"Escalado\", StandardScaler()), \n        (\"Regresi√≥n Log√≠stica\", LogisticRegression(max_iter=10000))]\npipe = Pipeline(steps)"
  },
  {
    "objectID": "clasification.html#entrenar-al-modelo",
    "href": "clasification.html#entrenar-al-modelo",
    "title": "Clasification",
    "section": "5. entrenar al modelo",
    "text": "5. entrenar al modelo\n\n\nCode\npipe.fit(X_train, y_train)\n\n\nPipeline(steps=[('Escalado', StandardScaler()),\n                ('Regresi√≥n Log√≠stica', LogisticRegression(max_iter=10000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('Escalado', ...), ('Regresi√≥n Log√≠stica', ...)]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy¬†\nTrue\n\n\n\nwith_mean¬†\nTrue\n\n\n\nwith_std¬†\nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty¬†\n'l2'\n\n\n\ndual¬†\nFalse\n\n\n\ntol¬†\n0.0001\n\n\n\nC¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\nintercept_scaling¬†\n1\n\n\n\nclass_weight¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nsolver¬†\n'lbfgs'\n\n\n\nmax_iter¬†\n10000\n\n\n\nmulti_class¬†\n'deprecated'\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nn_jobs¬†\nNone\n\n\n\nl1_ratio¬†\nNone"
  },
  {
    "objectID": "clasification.html#predicci√≥n-1",
    "href": "clasification.html#predicci√≥n-1",
    "title": "Clasification",
    "section": "6. Predicci√≥n",
    "text": "6. Predicci√≥n\n\n\nCode\ny_pred_pipe = pipe.predict(X_test)"
  },
  {
    "objectID": "clasification.html#evaluaci√≥ncls-1",
    "href": "clasification.html#evaluaci√≥ncls-1",
    "title": "Clasification",
    "section": "7. Evaluaci√≥ncls",
    "text": "7. Evaluaci√≥ncls\n\n\nCode\naccuray = accuracy_score(y_test, y_pred_pipe)\nprecision = precision_score(y_test, y_pred_pipe)\nrecall = recall_score(y_test, y_pred_pipe)\nf1 = f1_score(y_test, y_pred_pipe)\n\nprint(\"M√©tricas ejecutadas con pipe\")\nprint(f\"accuray = {accuray}\")\nprint(f\"precision = {precision}\")\nprint(f\"recall = {recall}\")\nprint(f\"f1 = {f1}\")\n\n\nM√©tricas ejecutadas con pipe\naccuray = 0.9692982456140351\nprecision = 0.9625850340136054\nrecall = 0.9895104895104895\nf1 = 0.9758620689655172"
  },
  {
    "objectID": "clasification.html#matriz-de-confusi√≥n-1",
    "href": "clasification.html#matriz-de-confusi√≥n-1",
    "title": "Clasification",
    "section": "8. Matriz de confusi√≥n",
    "text": "8. Matriz de confusi√≥n\n\n\nCode\nConfusionMatrixDisplay.from_predictions (y_test, y_pred_pipe)"
  },
  {
    "objectID": "clasification.html#evaluaci√≥n",
    "href": "clasification.html#evaluaci√≥n",
    "title": "Clasification",
    "section": "",
    "text": "Code\naccuray = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\"accuray = {accuray}\")\nprint(f\"precision = {precision}\")\nprint(f\"recall = {recall}\")\nprint(f\"f1 = {f1}\")\n\n\naccuray = 0.9912280701754386\nprecision = 0.9863013698630136\nrecall = 1.0\nf1 = 0.993103448275862"
  },
  {
    "objectID": "clasification.html#evaluaci√≥n-1",
    "href": "clasification.html#evaluaci√≥n-1",
    "title": "Clasification",
    "section": "7. Evaluaci√≥n",
    "text": "7. Evaluaci√≥n\n\n\nCode\naccuray = accuracy_score(y_test, y_pred_pipe)\nprecision = precision_score(y_test, y_pred_pipe)\nrecall = recall_score(y_test, y_pred_pipe)\nf1 = f1_score(y_test, y_pred_pipe)\n\nprint(\"M√©tricas ejecutadas con pipe\")\nprint(f\"accuray = {accuray}\")\nprint(f\"precision = {precision}\")\nprint(f\"recall = {recall}\")\nprint(f\"f1 = {f1}\")\n\n\nM√©tricas ejecutadas con pipe\naccuray = 1.0\nprecision = 1.0\nrecall = 1.0\nf1 = 1.0"
  },
  {
    "objectID": "midterm_regresion.html",
    "href": "midterm_regresion.html",
    "title": "MidTerm Regresi√≥n",
    "section": "",
    "text": "Este documento presenta un an√°lisis completo de regresi√≥n lineal utilizando el dataset iige003_carapungo.csv. Utilizaremos Pipeline de scikit-learn para crear un flujo de trabajo robusto y reproducible.\n\n\n\nNo: N√∫mero secuencial de la observaci√≥n.\nDate: Fecha de ka observaci√≥n.\nTime: Hora de la observaci√≥n.\nColdJunc0: Temperatura de la uni√≥n fr√≠a (referencia para sensores tipo termopar).\nPowerVolt: Voltaje de alimentaci√≥n del sistema o sensor.\nPowerKind: Tipo de fuente de energ√≠a (por ejemplo, solar, bater√≠a, red el√©ctrica).\nWS(ave): Velocidad promedio del viento (Wind Speed average), usualmente en m/s.\nWD(ave): Direcci√≥n promedio del viento (Wind Direction average), en grados.\nMax_time: Tiempo (hora/minuto) en el que se registr√≥ la velocidad m√°xima del viento (WS(max)) durante el periodo de medici√≥n.\nWS(max): Velocidad m√°xima del viento registrada en el periodo, en m/s.\nWD(most): Direcci√≥n del viento m√°s frecuente (Wind Direction most), es decir, la direcci√≥n en la que el viento sopl√≥ la mayor parte del tiempo durante el periodo de medici√≥n.\nWS(inst_m): Velocidad instant√°nea m√°xima del viento (Wind Speed instantaneous max), en m/s.\nWD(inst_m): Direcci√≥n instant√°nea m√°xima del viento, en grados.\nSolar_rad: Radiaci√≥n solar, normalmente en W/m¬≤.\nTEMP: Temperatura del aire, en ¬∞C.\nHumidity: Humedad relativa del aire, en %.\nRainfall: Precipitaci√≥n acumulada, en mm.\nBar_press.: Presi√≥n barom√©trica (atmosf√©rica), en hPa o mbar."
  },
  {
    "objectID": "midterm_regresion.html#introducci√≥n",
    "href": "midterm_regresion.html#introducci√≥n",
    "title": "MidTerm Regresi√≥n",
    "section": "",
    "text": "Este documento presenta un an√°lisis completo de regresi√≥n lineal utilizando el dataset iige003_carapungo.csv. Utilizaremos Pipeline de scikit-learn para crear un flujo de trabajo robusto y reproducible.\n\n\n\nNo: N√∫mero secuencial de la observaci√≥n.\nDate: Fecha de ka observaci√≥n.\nTime: Hora de la observaci√≥n.\nColdJunc0: Temperatura de la uni√≥n fr√≠a (referencia para sensores tipo termopar).\nPowerVolt: Voltaje de alimentaci√≥n del sistema o sensor.\nPowerKind: Tipo de fuente de energ√≠a (por ejemplo, solar, bater√≠a, red el√©ctrica).\nWS(ave): Velocidad promedio del viento (Wind Speed average), usualmente en m/s.\nWD(ave): Direcci√≥n promedio del viento (Wind Direction average), en grados.\nMax_time: Tiempo (hora/minuto) en el que se registr√≥ la velocidad m√°xima del viento (WS(max)) durante el periodo de medici√≥n.\nWS(max): Velocidad m√°xima del viento registrada en el periodo, en m/s.\nWD(most): Direcci√≥n del viento m√°s frecuente (Wind Direction most), es decir, la direcci√≥n en la que el viento sopl√≥ la mayor parte del tiempo durante el periodo de medici√≥n.\nWS(inst_m): Velocidad instant√°nea m√°xima del viento (Wind Speed instantaneous max), en m/s.\nWD(inst_m): Direcci√≥n instant√°nea m√°xima del viento, en grados.\nSolar_rad: Radiaci√≥n solar, normalmente en W/m¬≤.\nTEMP: Temperatura del aire, en ¬∞C.\nHumidity: Humedad relativa del aire, en %.\nRainfall: Precipitaci√≥n acumulada, en mm.\nBar_press.: Presi√≥n barom√©trica (atmosf√©rica), en hPa o mbar."
  },
  {
    "objectID": "midterm_regresion.html#importar-librer√≠as",
    "href": "midterm_regresion.html#importar-librer√≠as",
    "title": "MidTerm Regresi√≥n",
    "section": "1. Importar Librer√≠as",
    "text": "1. Importar Librer√≠as\nPrimero importamos todas las librer√≠as necesarias para el an√°lisis.\n\n\nCode\n# Librer√≠as para manipulaci√≥n de datos\nimport pandas as pd\nimport numpy as np\n\n# Librer√≠as para visualizaci√≥n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Librer√≠as de scikit-learn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Configuraci√≥n de estilo para gr√°ficos\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")"
  },
  {
    "objectID": "midterm_regresion.html#cargar-el-dataset",
    "href": "midterm_regresion.html#cargar-el-dataset",
    "title": "MidTerm Regresi√≥n",
    "section": "2. Cargar el Dataset",
    "text": "2. Cargar el Dataset\nCargamos el dataset desde el archivo CSV.\n\n\nCode\n# Cargar los datos\ndf = pd.read_csv(\"iige003_carapungo.csv\", encoding=\"latin1\")\n\n# Mostrar informaci√≥n b√°sica\nprint(f\"Dimensiones del dataset: {df.shape}\")\nprint(f\"N√∫mero de filas: {df.shape[0]}\")\nprint(f\"N√∫mero de columnas: {df.shape[1]}\")\n\n\nDimensiones del dataset: (133754, 18)\nN√∫mero de filas: 133754\nN√∫mero de columnas: 18\n\n\nC:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19092\\2803193322.py:2: DtypeWarning: Columns (3,4,6,7,8,10,11,13,14,15,16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"iige003_carapungo.csv\", encoding=\"latin1\")"
  },
  {
    "objectID": "midterm_regresion.html#exploraci√≥n-inicial-de-los-datos",
    "href": "midterm_regresion.html#exploraci√≥n-inicial-de-los-datos",
    "title": "MidTerm Regresi√≥n",
    "section": "3. Exploraci√≥n Inicial de los Datos",
    "text": "3. Exploraci√≥n Inicial de los Datos\n\n3.1 Primeras filas del dataset\n\n\nCode\n# Visualizar las primeras 5 filas\ndf.head()\n\n\n\n\n\n\n\n\n\nNo\nDate\nTime\nColdJunc0\nPowerVolt\nPowerKind\nWS(ave)\nWD(ave)\nWS(max)\nWD(most)\nWS(inst_m)\nWD(inst_m)\nMax_time\nSolar_rad\nTEMP\nHumidity\nRainfall\nBar_press.\n\n\n\n\n0\nNaN\nyyyy/mm/dd\nhh:mm:ss\n√üC\nV\nNaN\nm/s\n√ü\nm/s\nNaN\nm/s\n√ü\nNaN\nkW\n√üC\n%\nmm\nhPa\n\n\n1\n5.0\n2019/05/07\n13:10:00\n24.2\n12.9\n2.0\n0.8\n76\n0.8\nENE\n2.5\n82\n13:05:31\n0.325\n17.7\n61.65\n0\n743.2\n\n\n2\n6.0\n2019/05/07\n13:20:00\n24.3\n12.9\n2.0\n1.2\n65\n1.2\nENE\n3.1\n71\n13:14:36\n0.588\n17.7\n64.34\n0\n743.1\n\n\n3\n7.0\n2019/05/07\n13:30:00\n25.1\n12.9\n2.0\n0.8\n42\n1.3\nNE\n3.7\n44\n13:22:58\n0.815\n19.2\n57.54\n0\n743.1\n\n\n4\n8.0\n2019/05/07\n13:40:00\n25.6\n13.3\n2.0\n0.7\n49\n0.9\nNE\n3.1\n47\n13:32:59\n0.605\n19.4\n65.14\n0\n743.1\n\n\n\n\n\n\n\n\n\n3.2 Informaci√≥n del dataset\n\n\nCode\n# Informaci√≥n general del dataset\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 133754 entries, 0 to 133753\nData columns (total 18 columns):\n #   Column      Non-Null Count   Dtype  \n---  ------      --------------   -----  \n 0   No          133753 non-null  float64\n 1   Date        133754 non-null  object \n 2   Time        133754 non-null  object \n 3   ColdJunc0   133754 non-null  object \n 4   PowerVolt   133754 non-null  object \n 5   PowerKind   133753 non-null  float64\n 6   WS(ave)     133754 non-null  object \n 7   WD(ave)     133754 non-null  object \n 8   WS(max)     133754 non-null  object \n 9   WD(most)    133753 non-null  object \n 10  WS(inst_m)  133754 non-null  object \n 11  WD(inst_m)  133754 non-null  object \n 12  Max_time    133753 non-null  object \n 13  Solar_rad   133754 non-null  object \n 14  TEMP        133754 non-null  object \n 15  Humidity    133754 non-null  object \n 16  Rainfall    133754 non-null  object \n 17  Bar_press.  133754 non-null  object \ndtypes: float64(2), object(16)\nmemory usage: 18.4+ MB\n\n\n\n\n3.3 Transaformar columnas\n\n\nCode\ncols_to_numeric = df.columns[3:]\ndf[cols_to_numeric] = df[cols_to_numeric].apply(pd.to_numeric, errors='coerce')\n\n\n\n\n3.4 Eliminar columnas que no aportan\nSe elimina las columnas que no aportanin fromaci√≥n.\n\n\nCode\n# Eliminar las tres primeras columnas\ndf = df.iloc[:, 3:]\n\n# Eliminar columnas espec√≠ficas por nombre\ndf = df.drop(columns=[\"WD(most)\", \"Max_time\"], errors=\"ignore\")\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 133754 entries, 0 to 133753\nData columns (total 13 columns):\n #   Column      Non-Null Count   Dtype  \n---  ------      --------------   -----  \n 0   ColdJunc0   133753 non-null  float64\n 1   PowerVolt   133753 non-null  float64\n 2   PowerKind   133753 non-null  float64\n 3   WS(ave)     133753 non-null  float64\n 4   WD(ave)     133753 non-null  float64\n 5   WS(max)     133753 non-null  float64\n 6   WS(inst_m)  133753 non-null  float64\n 7   WD(inst_m)  133753 non-null  float64\n 8   Solar_rad   133753 non-null  float64\n 9   TEMP        133753 non-null  float64\n 10  Humidity    133753 non-null  float64\n 11  Rainfall    133753 non-null  float64\n 12  Bar_press.  133753 non-null  float64\ndtypes: float64(13)\nmemory usage: 13.3 MB\n\n\n\n\n3.5 Estad√≠sticas descriptivas\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\nColdJunc0\nPowerVolt\nPowerKind\nWS(ave)\nWD(ave)\nWS(max)\nWS(inst_m)\nWD(inst_m)\nSolar_rad\nTEMP\nHumidity\nRainfall\nBar_press.\n\n\n\n\ncount\n133753.000000\n133753.000000\n133753.0\n133753.000000\n133753.000000\n133753.000000\n133753.000000\n133753.000000\n133753.000000\n133753.000000\n133753.000000\n133753.000000\n133753.000000\n\n\nmean\n15.874370\n13.339876\n2.0\n0.322607\n147.440446\n0.418731\n1.668615\n151.121051\n0.174373\n13.485849\n83.733397\n0.009499\n742.666402\n\n\nstd\n6.815222\n0.458272\n0.0\n0.482661\n91.021558\n0.560030\n1.675281\n90.558364\n0.276295\n3.702440\n15.835706\n0.110822\n1.609745\n\n\nmin\n-1.100000\n12.500000\n2.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.200000\n15.680000\n0.000000\n736.900000\n\n\n25%\n11.000000\n13.000000\n2.0\n0.000000\n61.000000\n0.000000\n0.000000\n72.000000\n0.000000\n10.900000\n72.530000\n0.000000\n741.600000\n\n\n50%\n13.200000\n13.100000\n2.0\n0.100000\n148.000000\n0.200000\n1.300000\n162.000000\n0.002000\n12.600000\n89.910000\n0.000000\n742.700000\n\n\n75%\n20.800000\n13.700000\n2.0\n0.500000\n224.000000\n0.700000\n2.500000\n228.000000\n0.265000\n16.100000\n97.590000\n0.000000\n743.800000\n\n\nmax\n38.200000\n15.000000\n2.0\n5.000000\n360.000000\n5.100000\n25.300000\n355.000000\n1.427000\n26.400000\n99.380000\n9.500000\n748.400000\n\n\n\n\n\n\n\n\n\n3.6 Asignar valores medios a los nulos\n\n\nCode\n# Reemplazar valores NaN en columnas num√©ricas por la media de cada columna\ndf = df.fillna(df.mean(numeric_only=True))\n\n\n\n\n3.6 Valores nulos\n\n\nCode\n# Verificar valores nulos por columna\nvalores_nulos = df.isnull().sum()\nprint(\"Valores nulos por columna:\")\nprint(valores_nulos)\nprint(f\"\\nTotal de valores nulos: {valores_nulos.sum()}\")\n\n\nValores nulos por columna:\nColdJunc0     0\nPowerVolt     0\nPowerKind     0\nWS(ave)       0\nWD(ave)       0\nWS(max)       0\nWS(inst_m)    0\nWD(inst_m)    0\nSolar_rad     0\nTEMP          0\nHumidity      0\nRainfall      0\nBar_press.    0\ndtype: int64\n\nTotal de valores nulos: 0"
  },
  {
    "objectID": "midterm_regresion.html#an√°lisis-exploratorio-de-datos-eda",
    "href": "midterm_regresion.html#an√°lisis-exploratorio-de-datos-eda",
    "title": "MidTerm Regresi√≥n",
    "section": "4. An√°lisis Exploratorio de Datos (EDA)",
    "text": "4. An√°lisis Exploratorio de Datos (EDA)\n\n4.1 Distribuci√≥n de las variables\nVisualizamos la distribuci√≥n de todas las variables num√©ricas mediante histogramas.\n\n\nCode\n# Crear histogramas para todas las variables num√©ricas\ndf.hist(figsize=(12, 8), bins=20, edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: Distribuci√≥n de las variables num√©ricas del dataset\n\n\n\n\n\n\n\n4.2 Matriz de correlaci√≥n\nLa matriz de correlaci√≥n nos ayuda a identificar relaciones lineales entre variables.\n\n\nCode\n# Crear matriz de correlaci√≥n\nplt.figure(figsize=(10, 8))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f', \n            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\nplt.title('Matriz de Correlaci√≥n', fontsize=16, pad=20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†2: Matriz de correlaci√≥n entre variables\n\n\n\n\n\n\n\n4.3 An√°lisis de correlaciones fuertes\n\n\nCode\n# Encontrar correlaciones fuertes (&gt;0.7 o &lt;-0.7)\ncorr_matrix = df.corr()\ncorrelaciones_fuertes = []\n\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        if abs(corr_matrix.iloc[i, j]) &gt; 0.7:\n            correlaciones_fuertes.append({\n                'Variable 1': corr_matrix.columns[i],\n                'Variable 2': corr_matrix.columns[j],\n                'Correlaci√≥n': corr_matrix.iloc[i, j]\n            })\n\nif correlaciones_fuertes:\n    print(\"Correlaciones fuertes encontradas (|r| &gt; 0.7):\")\n    print(pd.DataFrame(correlaciones_fuertes))\nelse:\n    print(\"No se encontraron correlaciones fuertes (|r| &gt; 0.7)\")\n\n\nCorrelaciones fuertes encontradas (|r| &gt; 0.7):\n  Variable 1  Variable 2  Correlaci√≥n\n0  ColdJunc0   Solar_rad     0.806508\n1  ColdJunc0        TEMP     0.965260\n2  ColdJunc0    Humidity    -0.836064\n3    WS(ave)     WS(max)     0.953437\n4    WS(ave)  WS(inst_m)     0.887367\n5    WD(ave)  WD(inst_m)     0.752963\n6    WS(max)  WS(inst_m)     0.886897\n7  Solar_rad        TEMP     0.758202\n8  Solar_rad    Humidity    -0.712463\n9       TEMP    Humidity    -0.826535"
  },
  {
    "objectID": "midterm_regresion.html#preparaci√≥n-de-los-datos",
    "href": "midterm_regresion.html#preparaci√≥n-de-los-datos",
    "title": "MidTerm Regresi√≥n",
    "section": "5. Preparaci√≥n de los Datos",
    "text": "5. Preparaci√≥n de los Datos\nSeparamos las variables independientes (features) de la variable objetivo.\n\n\nCode\nvariable_objetivo = 'Rainfall'  \n\n# Separar features (X) y target (y)\nX = df.drop(variable_objetivo, axis=1)\ny = df[variable_objetivo]\n\nprint(f\"N√∫mero de features: {X.shape[1]}\")\nprint(f\"Features utilizadas: {list(X.columns)}\")\n\n\nN√∫mero de features: 12\nFeatures utilizadas: ['ColdJunc0', 'PowerVolt', 'PowerKind', 'WS(ave)', 'WD(ave)', 'WS(max)', 'WS(inst_m)', 'WD(inst_m)', 'Solar_rad', 'TEMP', 'Humidity', 'Bar_press.']"
  },
  {
    "objectID": "midterm_regresion.html#divisi√≥n-de-datos",
    "href": "midterm_regresion.html#divisi√≥n-de-datos",
    "title": "MidTerm Regresi√≥n",
    "section": "6. Divisi√≥n de Datos",
    "text": "6. Divisi√≥n de Datos\nDividimos el dataset en conjuntos de entrenamiento (80%) y prueba (20%).\n\n\nCode\n# Dividir los datos\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"Tama√±o del conjunto de entrenamiento: {X_train.shape[0]} muestras\")\nprint(f\"Tama√±o del conjunto de prueba: {X_test.shape[0]} muestras\")\nprint(f\"Proporci√≥n: {X_train.shape[0]/len(df)*100:.1f}% entrenamiento, {X_test.shape[0]/len(df)*100:.1f}% prueba\")\n\n\nTama√±o del conjunto de entrenamiento: 107003 muestras\nTama√±o del conjunto de prueba: 26751 muestras\nProporci√≥n: 80.0% entrenamiento, 20.0% prueba"
  },
  {
    "objectID": "midterm_regresion.html#creaci√≥n-del-pipeline",
    "href": "midterm_regresion.html#creaci√≥n-del-pipeline",
    "title": "MidTerm Regresi√≥n",
    "section": "7. Creaci√≥n del Pipeline",
    "text": "7. Creaci√≥n del Pipeline\nCreamos un pipeline que incluye: 1. StandardScaler: Normaliza los datos (media=0, desviaci√≥n est√°ndar=1) 2. LinearRegression: Aplica el modelo de regresi√≥n lineal\n\n\nCode\n# Crear el pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),      # Paso 1: Normalizaci√≥n\n    ('regressor', LinearRegression())  # Paso 2: Regresi√≥n Lineal\n])\n\nprint(\"Pipeline creado con los siguientes pasos:\")\nfor nombre, paso in pipeline.steps:\n    print(f\"  - {nombre}: {paso.__class__.__name__}\")\n\n\nPipeline creado con los siguientes pasos:\n  - scaler: StandardScaler\n  - regressor: LinearRegression"
  },
  {
    "objectID": "midterm_regresion.html#entrenamiento-del-modelo",
    "href": "midterm_regresion.html#entrenamiento-del-modelo",
    "title": "MidTerm Regresi√≥n",
    "section": "8. Entrenamiento del Modelo",
    "text": "8. Entrenamiento del Modelo\nEntrenamos el pipeline completo con los datos de entrenamiento.\n\n\nCode\n# Entrenar el pipeline\npipeline.fit(X_train, y_train)\nprint(\"‚úì Modelo entrenado exitosamente\")\n\n\n‚úì Modelo entrenado exitosamente"
  },
  {
    "objectID": "midterm_regresion.html#predicciones",
    "href": "midterm_regresion.html#predicciones",
    "title": "MidTerm Regresi√≥n",
    "section": "9. Predicciones",
    "text": "9. Predicciones\nRealizamos predicciones con el conjunto de prueba.\n\n\nCode\n# Realizar predicciones\ny_pred = pipeline.predict(X_test)\n\n# Mostrar las primeras 10 predicciones vs valores reales\ncomparacion = pd.DataFrame({\n    'Valor Real': y_test.values[:10],\n    'Predicci√≥n': y_pred[:10],\n    'Error': y_test.values[:10] - y_pred[:10]\n})\nprint(\"Primeras 10 predicciones:\")\nprint(comparacion)\n\n\nPrimeras 10 predicciones:\n   Valor Real  Predicci√≥n     Error\n0         0.0    0.005687 -0.005687\n1         0.0    0.030682 -0.030682\n2         0.0   -0.007404  0.007404\n3         0.0    0.017868 -0.017868\n4         0.0   -0.006223  0.006223\n5         0.0    0.014569 -0.014569\n6         0.0   -0.008298  0.008298\n7         0.0    0.014926 -0.014926\n8         0.0   -0.016371  0.016371\n9         0.0   -0.028041  0.028041"
  },
  {
    "objectID": "midterm_regresion.html#evaluaci√≥n-del-modelo",
    "href": "midterm_regresion.html#evaluaci√≥n-del-modelo",
    "title": "MidTerm Regresi√≥n",
    "section": "10. Evaluaci√≥n del Modelo",
    "text": "10. Evaluaci√≥n del Modelo\n\n10.1 M√©tricas de desempe√±o\n\n\nCode\n# Calcular m√©tricas\nr2 = r2_score(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\n\n# Mostrar resultados\nprint(\"=\"*60)\nprint(\"M√âTRICAS DE EVALUACI√ìN DEL MODELO\")\nprint(\"=\"*60)\nprint(f\"R¬≤ Score:  {r2:.4f}  ‚Üí Explica el {r2*100:.2f}% de la variabilidad\")\nprint(f\"MSE:       {mse:.4f}  ‚Üí Error cuadr√°tico medio\")\nprint(f\"RMSE:      {rmse:.4f}  ‚Üí Ra√≠z del error cuadr√°tico medio\")\nprint(f\"MAE:       {mae:.4f}  ‚Üí Error absoluto medio\")\nprint(\"=\"*60)\n\n# Interpretaci√≥n del R¬≤\nif r2 &gt; 0.9:\n    print(\"Interpretaci√≥n: Excelente ajuste del modelo\")\nelif r2 &gt; 0.7:\n    print(\"Interpretaci√≥n: Buen ajuste del modelo\")\nelif r2 &gt; 0.5:\n    print(\"Interpretaci√≥n: Ajuste moderado del modelo\")\nelse:\n    print(\"Interpretaci√≥n: Ajuste d√©bil del modelo\")\n\n\n============================================================\nM√âTRICAS DE EVALUACI√ìN DEL MODELO\n============================================================\nR¬≤ Score:  0.0164  ‚Üí Explica el 1.64% de la variabilidad\nMSE:       0.0136  ‚Üí Error cuadr√°tico medio\nRMSE:      0.1165  ‚Üí Ra√≠z del error cuadr√°tico medio\nMAE:       0.0231  ‚Üí Error absoluto medio\n============================================================\nInterpretaci√≥n: Ajuste d√©bil del modelo\n\n\n\n\n10.2 Validaci√≥n cruzada\nEvaluamos el modelo con validaci√≥n cruzada de 5 particiones.\n\n\nCode\n# Validaci√≥n cruzada\ncv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n\nprint(\"\\nVALIDACI√ìN CRUZADA (5-fold)\")\nprint(\"=\"*60)\nprint(f\"R¬≤ scores por fold: {cv_scores}\")\nprint(f\"R¬≤ promedio: {cv_scores.mean():.4f}\")\nprint(f\"Desviaci√≥n est√°ndar: {cv_scores.std():.4f}\")\nprint(f\"Rango: [{cv_scores.min():.4f}, {cv_scores.max():.4f}]\")\n\n\n\nVALIDACI√ìN CRUZADA (5-fold)\n============================================================\nR¬≤ scores por fold: [0.01748701 0.01839957 0.01676307 0.01694792 0.0154534 ]\nR¬≤ promedio: 0.0170\nDesviaci√≥n est√°ndar: 0.0010\nRango: [0.0155, 0.0184]"
  },
  {
    "objectID": "midterm_regresion.html#visualizaci√≥n-de-resultados",
    "href": "midterm_regresion.html#visualizaci√≥n-de-resultados",
    "title": "MidTerm Regresi√≥n",
    "section": "11. Visualizaci√≥n de Resultados",
    "text": "11. Visualizaci√≥n de Resultados\n\n11.1 Valores reales vs predicciones\nEste gr√°fico muestra qu√© tan cerca est√°n las predicciones de los valores reales.\n\n\nCode\n# Gr√°fico de dispersi√≥n\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, alpha=0.6, edgecolors='k', s=80)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n         'r--', lw=3, label='Predicci√≥n perfecta')\nplt.xlabel('Valores Reales', fontsize=13)\nplt.ylabel('Predicciones', fontsize=13)\nplt.title('Valores Reales vs Predicciones', fontsize=15, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†3: Comparaci√≥n entre valores reales y predicciones del modelo\n\n\n\n\n\n\n\n11.2 Gr√°fico de residuos\nLos residuos son los errores del modelo (diferencia entre valor real y predicci√≥n).\n\n\nCode\n# Calcular residuos\nresiduos = y_test - y_pred\n\n# Gr√°fico de residuos\nplt.figure(figsize=(10, 6))\nplt.scatter(y_pred, residuos, alpha=0.6, edgecolors='k', s=80)\nplt.axhline(y=0, color='r', linestyle='--', lw=3, label='Residuo = 0')\nplt.xlabel('Predicciones', fontsize=13)\nplt.ylabel('Residuos', fontsize=13)\nplt.title('Gr√°fico de Residuos', fontsize=15, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Estad√≠sticas de residuos\nprint(f\"Media de residuos: {residuos.mean():.4f} (idealmente cerca de 0)\")\nprint(f\"Desviaci√≥n est√°ndar de residuos: {residuos.std():.4f}\")\n\n\n\n\n\n\n\n\nFigure¬†4: An√°lisis de residuos del modelo\n\n\n\n\n\nMedia de residuos: 0.0006 (idealmente cerca de 0)\nDesviaci√≥n est√°ndar de residuos: 0.1165\n\n\n\n\n11.3 Distribuci√≥n de residuos\n\n\nCode\n# Histograma de residuos\nplt.figure(figsize=(10, 5))\nplt.hist(residuos, bins=30, edgecolor='black', alpha=0.7)\nplt.axvline(x=0, color='r', linestyle='--', lw=2, label='Residuo = 0')\nplt.xlabel('Residuos', fontsize=13)\nplt.ylabel('Frecuencia', fontsize=13)\nplt.title('Distribuci√≥n de Residuos', fontsize=15, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†5: Distribuci√≥n de los residuos"
  },
  {
    "objectID": "midterm_regresion.html#coeficientes-del-modelo",
    "href": "midterm_regresion.html#coeficientes-del-modelo",
    "title": "MidTerm Regresi√≥n",
    "section": "12. Coeficientes del Modelo",
    "text": "12. Coeficientes del Modelo\nLos coeficientes indican la importancia y direcci√≥n del efecto de cada variable.\n\n12.1 Tabla de coeficientes\n\n\nCode\n# Extraer el modelo del pipeline\nregressor = pipeline.named_steps['regressor']\n\n# Crear DataFrame con coeficientes\ncoef_df = pd.DataFrame({\n    'Variable': X.columns,\n    'Coeficiente': regressor.coef_\n}).sort_values('Coeficiente', key=abs, ascending=False)\n\nprint(\"COEFICIENTES DEL MODELO\")\nprint(\"=\"*60)\nprint(f\"Intercepto: {regressor.intercept_:.4f}\\n\")\nprint(\"Coeficientes por variable (ordenados por magnitud):\")\nprint(coef_df.to_string(index=False))\n\n\nCOEFICIENTES DEL MODELO\n============================================================\nIntercepto: 0.0094\n\nCoeficientes por variable (ordenados por magnitud):\n  Variable   Coeficiente\n ColdJunc0  3.230625e-02\n  Humidity  2.238645e-02\n      TEMP -1.479549e-02\nWS(inst_m)  9.502297e-03\n Solar_rad -8.718406e-03\n   WS(ave) -5.600328e-03\n   WS(max)  4.435772e-03\n PowerVolt  1.920716e-03\nBar_press.  1.853470e-03\nWD(inst_m) -1.283488e-03\n   WD(ave)  1.144717e-03\n PowerKind -2.081668e-17\n\n\n\n\n12.2 Visualizaci√≥n de coeficientes\n\n\nCode\n# Gr√°fico de barras horizontales\nplt.figure(figsize=(10, 6))\ncolors = ['green' if x &gt; 0 else 'red' for x in coef_df['Coeficiente']]\nplt.barh(coef_df['Variable'], coef_df['Coeficiente'], color=colors, alpha=0.7)\nplt.xlabel('Coeficiente', fontsize=13)\nplt.ylabel('Variable', fontsize=13)\nplt.title('Importancia de las Variables (Coeficientes)', fontsize=15, fontweight='bold')\nplt.axvline(x=0, color='black', linestyle='-', linewidth=1)\nplt.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†6: Importancia de las variables seg√∫n sus coeficientes\n\n\n\n\n\n\n\n12.3 Interpretaci√≥n de coeficientes\n\n\nCode\nprint(\"\\nINTERPRETACI√ìN DE COEFICIENTES:\")\nprint(\"=\"*60)\nprint(\"Coeficientes positivos ‚Üí Aumentan el valor de la variable objetivo\")\nprint(\"Coeficientes negativos ‚Üí Disminuyen el valor de la variable objetivo\")\nprint(\"Mayor magnitud ‚Üí Mayor impacto en la predicci√≥n\\n\")\n\n# Variables con mayor impacto\nprint(\"Top 3 variables con mayor impacto (valor absoluto):\")\nfor idx, row in coef_df.head(3).iterrows():\n    direccion = \"aumenta\" if row['Coeficiente'] &gt; 0 else \"disminuye\"\n    print(f\"  {row['Variable']}: {direccion} la predicci√≥n en {abs(row['Coeficiente']):.4f}\")\n\n\n\nINTERPRETACI√ìN DE COEFICIENTES:\n============================================================\nCoeficientes positivos ‚Üí Aumentan el valor de la variable objetivo\nCoeficientes negativos ‚Üí Disminuyen el valor de la variable objetivo\nMayor magnitud ‚Üí Mayor impacto en la predicci√≥n\n\nTop 3 variables con mayor impacto (valor absoluto):\n  ColdJunc0: aumenta la predicci√≥n en 0.0323\n  Humidity: aumenta la predicci√≥n en 0.0224\n  TEMP: disminuye la predicci√≥n en 0.0148"
  },
  {
    "objectID": "midterm_regresion.html#conclusiones",
    "href": "midterm_regresion.html#conclusiones",
    "title": "MidTerm Regresi√≥n",
    "section": "13. Conclusiones",
    "text": "13. Conclusiones\n\n\nCode\nprint(\"RESUMEN DEL AN√ÅLISIS\")\nprint(\"=\"*60)\nprint(f\"‚úì Dataset: {df.shape[0]} muestras, {df.shape[1]} variables\")\nprint(f\"‚úì Modelo: Regresi√≥n Lineal con Pipeline\")\nprint(f\"‚úì R¬≤ Score: {r2:.4f}\")\nprint(f\"‚úì RMSE: {rmse:.4f}\")\nprint(f\"‚úì Validaci√≥n cruzada (5-fold): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\nprint(\"=\"*60)\n\n\nRESUMEN DEL AN√ÅLISIS\n============================================================\n‚úì Dataset: 133754 muestras, 13 variables\n‚úì Modelo: Regresi√≥n Lineal con Pipeline\n‚úì R¬≤ Score: 0.0164\n‚úì RMSE: 0.1165\n‚úì Validaci√≥n cruzada (5-fold): 0.0170 ¬± 0.0010\n============================================================"
  },
  {
    "objectID": "midterm_regresion.html#notas-importantes",
    "href": "midterm_regresion.html#notas-importantes",
    "title": "MidTerm Regresi√≥n",
    "section": "Notas Importantes",
    "text": "Notas Importantes\n\nVariable objetivo: La variable objetivo es: Rainfall.\nPreprocesamiento: El pipeline incluye StandardScaler para normalizar los datos.\nInterpretaci√≥n: Los coeficientes est√°n en escala estandarizada debido al StandardScaler.\n\n\nInterpretaci√≥n Final üß†\n\nEl modelo de regresi√≥n lineal desarrollado permite predecir la precipitaci√≥n acumulada (Rainfall) a partir de variables meteorol√≥gicas medidas en la estaci√≥n.\nEl an√°lisis de los coeficientes revela qu√© variables tienen mayor impacto en la predicci√≥n de la lluvia. Variables como la humedad, la temperatura y la radiaci√≥n solar suelen ser determinantes en los procesos de precipitaci√≥n, lo que se refleja en la magnitud y el signo de sus coeficientes.\nEn conclusi√≥n, el modelo es una herramienta √∫til para estimar la precipitaci√≥n a partir de datos meteorol√≥gicos, aunque su precisi√≥n depende de la calidad y representatividad de los datos. Se recomienda complementar este an√°lisis con modelos m√°s complejos o incorporar m√°s datos para mejorar la capacidad predictiva si es necesario."
  },
  {
    "objectID": "midterm_clasificacion.html",
    "href": "midterm_clasificacion.html",
    "title": "MidTerm Clasificaci√≥n",
    "section": "",
    "text": "Este documento presenta un an√°lisis completo de regresi√≥n lineal utilizando el dataset AcademicStressLevel.csv. Utilizaremos Pipeline de scikit-learn para crear un flujo de trabajo robusto y reproducible."
  },
  {
    "objectID": "midterm_clasificacion.html#introducci√≥n",
    "href": "midterm_clasificacion.html#introducci√≥n",
    "title": "MidTerm Clasificaci√≥n",
    "section": "",
    "text": "Este documento presenta un an√°lisis completo de regresi√≥n lineal utilizando el dataset AcademicStressLevel.csv. Utilizaremos Pipeline de scikit-learn para crear un flujo de trabajo robusto y reproducible."
  },
  {
    "objectID": "midterm_clasificacion.html#importar-librer√≠as",
    "href": "midterm_clasificacion.html#importar-librer√≠as",
    "title": "MidTerm Clasificaci√≥n",
    "section": "1. Importar Librer√≠as",
    "text": "1. Importar Librer√≠as\nPrimero importamos todas las librer√≠as necesarias para el an√°lisis.\n\n\nCode\n# Librer√≠as para manipulaci√≥n de datos\nimport pandas as pd\nimport numpy as np\n\n# Librer√≠as para visualizaci√≥n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Librer√≠as de scikit-learn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Configuraci√≥n de estilo para gr√°ficos\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")"
  },
  {
    "objectID": "midterm_clasificacion.html#carga-y-exploraci√≥n-inicial-del-dataset",
    "href": "midterm_clasificacion.html#carga-y-exploraci√≥n-inicial-del-dataset",
    "title": "MidTerm Clasificaci√≥n",
    "section": "1. Carga y Exploraci√≥n Inicial del Dataset",
    "text": "1. Carga y Exploraci√≥n Inicial del Dataset\nPrimero, cargamos las librer√≠as necesarias y el conjunto de datos. Realizaremos una exploraci√≥n b√°sica para entender su estructura, identificar las variables num√©ricas y categ√≥ricas, y prepararlo para el modelo.\n\n\nCode\n# Cargar el dataset desde el archivo CSV\nfile_path = 'AcademicStressLevel.csv'\ndf = pd.read_csv(file_path)\n\n# --- Exploraci√≥n Inicial ---\nprint(\"--- Informaci√≥n General del Dataset ---\")\ndf.info()\n\nprint(\"\\n--- Primeras 5 Filas del Dataset ---\")\nprint(df.head())\n\n# --- Limpieza y Selecci√≥n de Caracter√≠sticas ---\n# Limpiar nombres de columnas antes de renombrar\ndf.columns = df.columns.str.strip()\n\n# Los nombres de las columnas son largos y contienen espacios. Vamos a renombrarlos.\ncolumn_mapping = {\n    'Your Academic Stage': 'academic_stage',\n    'Peer pressure': 'peer_pressure',\n    'Academic pressure from your home': 'home_pressure',\n    'Study Environment': 'study_environment',\n    'What coping strategy you use as a student?': 'coping_strategy',\n    'Do you have any bad habits like smoking, drinking on a daily basis?': 'bad_habits',\n    'What would you rate the academic  competition in your student life': 'competition_rating',\n    'Rate your academic stress index': 'stress_index'\n}\ndf = df.rename(columns=column_mapping)\n\n\n--- Informaci√≥n General del Dataset ---\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 140 entries, 0 to 139\nData columns (total 9 columns):\n #   Column                                                               Non-Null Count  Dtype \n---  ------                                                               --------------  ----- \n 0   Timestamp                                                            140 non-null    object\n 1   Your Academic Stage                                                  140 non-null    object\n 2   Peer pressure                                                        140 non-null    int64 \n 3   Academic pressure from your home                                     140 non-null    int64 \n 4   Study Environment                                                    139 non-null    object\n 5   What coping strategy you use as a student?                           140 non-null    object\n 6   Do you have any bad habits like smoking, drinking on a daily basis?  140 non-null    object\n 7   What would you rate the academic  competition in your student life   140 non-null    int64 \n 8   Rate your academic stress index                                      140 non-null    int64 \ndtypes: int64(4), object(5)\nmemory usage: 10.0+ KB\n\n--- Primeras 5 Filas del Dataset ---\n             Timestamp Your Academic Stage  Peer pressure  \\\n0  24/07/2025 22:05:39       undergraduate              4   \n1  24/07/2025 22:05:52       undergraduate              3   \n2  24/07/2025 22:06:39       undergraduate              1   \n3  24/07/2025 22:06:45       undergraduate              3   \n4  24/07/2025 22:08:06       undergraduate              3   \n\n   Academic pressure from your home Study Environment  \\\n0                                 5             Noisy   \n1                                 4          Peaceful   \n2                                 1          Peaceful   \n3                                 2          Peaceful   \n4                                 3          Peaceful   \n\n          What coping strategy you use as a student?  \\\n0  Analyze the situation and handle it with intel...   \n1  Analyze the situation and handle it with intel...   \n2                   Social support (friends, family)   \n3  Analyze the situation and handle it with intel...   \n4  Analyze the situation and handle it with intel...   \n\n  Do you have any bad habits like smoking, drinking on a daily basis?  \\\n0                                                 No                    \n1                                                 No                    \n2                                                 No                    \n3                                                 No                    \n4                                                 No                    \n\n   What would you rate the academic  competition in your student life  \\\n0                                                  3                    \n1                                                  3                    \n2                                                  2                    \n3                                                  4                    \n4                                                  4                    \n\n   Rate your academic stress index   \n0                                 5  \n1                                 3  \n2                                 4  \n3                                 3  \n4                                 5  \n\n\nVer columnas renombradas\n\n\nCode\nprint(\"--- Informaci√≥n General del Dataset renombado ---\")\ndf.info()\n\n\n--- Informaci√≥n General del Dataset renombado ---\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 140 entries, 0 to 139\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   Timestamp           140 non-null    object\n 1   academic_stage      140 non-null    object\n 2   peer_pressure       140 non-null    int64 \n 3   home_pressure       140 non-null    int64 \n 4   study_environment   139 non-null    object\n 5   coping_strategy     140 non-null    object\n 6   bad_habits          140 non-null    object\n 7   competition_rating  140 non-null    int64 \n 8   stress_index        140 non-null    int64 \ndtypes: int64(4), object(5)\nmemory usage: 10.0+ KB\n\n\n\n\nCode\n# Seg√∫n los lineamientos, debemos evitar el uso de variables categ√≥ricas.\n# Identificamos las columnas categ√≥ricas a eliminar:\ncategorical_cols = ['academic_stage', 'study_environment', 'coping_strategy', 'bad_habits']\n# Tambi√©n eliminamos 'Timestamp' por no ser relevante para el modelo.\ncols_to_drop = categorical_cols + ['Timestamp']\n\ndf_cleaned = df.drop(columns=cols_to_drop)\n\n# Verificamos si hay valores nulos\nprint(f\"\\n--- Valores Nulos por Columna (despu√©s de limpiar) ---\")\nprint(df_cleaned.isnull().sum())\n\n# Si hubiera valores nulos, una opci√≥n ser√≠a eliminarlos.\n# df_cleaned = df_cleaned.dropna()\n\nprint(\"\\n--- Dataset Limpio (solo variables num√©ricas) ---\")\nprint(df_cleaned.head())\n\nprint(\"\\n--- Descripci√≥n Estad√≠stica del Dataset Limpio ---\")\nprint(df_cleaned.describe())\n\n\n\n--- Valores Nulos por Columna (despu√©s de limpiar) ---\npeer_pressure         0\nhome_pressure         0\ncompetition_rating    0\nstress_index          0\ndtype: int64\n\n--- Dataset Limpio (solo variables num√©ricas) ---\n   peer_pressure  home_pressure  competition_rating  stress_index\n0              4              5                   3             5\n1              3              4                   3             3\n2              1              1                   2             4\n3              3              2                   4             3\n4              3              3                   4             5\n\n--- Descripci√≥n Estad√≠stica del Dataset Limpio ---\n       peer_pressure  home_pressure  competition_rating  stress_index\ncount     140.000000     140.000000          140.000000    140.000000\nmean        3.071429       3.178571            3.492857      3.721429\nstd         1.083844       1.276618            1.028349      1.032339\nmin         1.000000       1.000000            1.000000      1.000000\n25%         2.000000       2.000000            3.000000      3.000000\n50%         3.000000       3.000000            4.000000      4.000000\n75%         4.000000       4.000000            4.000000      4.000000\nmax         5.000000       5.000000            5.000000      5.000000\n\n\nInterpretaci√≥n de la Exploraci√≥n: El dataset original contiene una mezcla de variables num√©ricas y categ√≥ricas. Para cumplir con los requisitos, hemos eliminado las columnas categ√≥ricas (academic_stage, study_environment, etc.) y la columna Timestamp. Nos quedamos con las siguientes variables num√©ricas: peer_pressure, home_pressure, competition_rating como nuestras caracter√≠sticas (features), y stress_index como nuestra variable objetivo (target). Afortunadamente, no se encontraron valores nulos en las columnas seleccionadas."
  },
  {
    "objectID": "midterm_clasificacion.html#divisi√≥n-en-conjuntos-de-entrenamiento-y-prueba",
    "href": "midterm_clasificacion.html#divisi√≥n-en-conjuntos-de-entrenamiento-y-prueba",
    "title": "MidTerm Clasificaci√≥n",
    "section": "4. Divisi√≥n en Conjuntos de Entrenamiento y Prueba",
    "text": "4. Divisi√≥n en Conjuntos de Entrenamiento y Prueba\nAhora, dividimos nuestro dataset limpio en dos conjuntos: uno para entrenar el modelo y otro para evaluarlo de manera imparcial. Usaremos una divisi√≥n 80/20, que es un est√°ndar com√∫n en la industria.\n\n\nCode\n# Definir las caracter√≠sticas (X) y la variable objetivo (y)\nX = df_cleaned.drop('stress_index', axis=1)\ny = df_cleaned['stress_index']\n\n# Dividir los datos en entrenamiento (80%) y prueba (20%)\n# Usamos random_state para que la divisi√≥n sea reproducible\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(\"--- Dimensiones de los Conjuntos de Datos ---\")\nprint(f\"Forma de X_train: {X_train.shape}\")\nprint(f\"Forma de X_test: {X_test.shape}\")\nprint(f\"Forma de y_train: {y_train.shape}\")\nprint(f\"Forma de y_test: {y_test.shape}\")\n\n\n--- Dimensiones de los Conjuntos de Datos ---\nForma de X_train: (112, 3)\nForma de X_test: (28, 3)\nForma de y_train: (112,)\nForma de y_test: (28,)\n\n\nNota: Se utiliz√≥ el par√°metro stratify=y para asegurar que la proporci√≥n de cada clase de estr√©s sea la misma tanto en el conjunto de entrenamiento como en el de prueba. Esto es crucial en problemas de clasificaci√≥n, especialmente si las clases est√°n desbalanceadas."
  },
  {
    "objectID": "midterm_clasificacion.html#definici√≥n-y-entrenamiento-del-modelo-utilizando-pipeline",
    "href": "midterm_clasificacion.html#definici√≥n-y-entrenamiento-del-modelo-utilizando-pipeline",
    "title": "MidTerm Clasificaci√≥n",
    "section": "5. Definici√≥n y Entrenamiento del Modelo utilizando Pipeline",
    "text": "5. Definici√≥n y Entrenamiento del Modelo utilizando Pipeline\nAqu√≠ creamos el Pipeline. Este objeto encapsula una secuencia de transformaciones y un estimador final. Nuestro pipeline constar√° de dos pasos:\n\nStandardScaler: Estandariza las caracter√≠sticas eliminando la media y escalando a la varianza unitaria. Es un paso fundamental para modelos como la Regresi√≥n Log√≠stica.\nLogisticRegression: El modelo de clasificaci√≥n que queremos entrenar.\n\nEl Pipeline se entrena con una sola llamada al m√©todo .fit().\n\n\nCode\n# Definir los pasos del pipeline\n# Paso 1: Escalar los datos\n# Paso 2: Aplicar el modelo de Regresi√≥n Log√≠stica\npipeline_steps = [\n    ('scaler', StandardScaler()),\n    ('logreg', LogisticRegression(random_state=42, multi_class='auto', solver='lbfgs'))\n]\n\n# Crear el pipeline\nmodel_pipeline = Pipeline(pipeline_steps)\n\n# Entrenar el pipeline completo con los datos de entrenamiento\nprint(\"--- Entrenando el Pipeline ---\")\nmodel_pipeline.fit(X_train, y_train)\nprint(\"¬°Entrenamiento completado!\")\n\n\n--- Entrenando el Pipeline ---\n¬°Entrenamiento completado!\n\n\nC:\\Personal\\Yachay\\machine_learning\\.venv-13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn("
  },
  {
    "objectID": "midterm_clasificacion.html#generaci√≥n-de-predicciones",
    "href": "midterm_clasificacion.html#generaci√≥n-de-predicciones",
    "title": "MidTerm Clasificaci√≥n",
    "section": "6. Generaci√≥n de Predicciones",
    "text": "6. Generaci√≥n de Predicciones\nUna vez que el pipeline est√° entrenado, lo usamos para hacer predicciones sobre el conjunto de prueba (X_test). El pipeline se encarga autom√°ticamente de aplicar la misma transformaci√≥n de escalado que aprendi√≥ de los datos de entrenamiento antes de pasar los datos al modelo para la predicci√≥n.\n\n\nCode\n# Realizar predicciones sobre el conjunto de prueba\ny_pred = model_pipeline.predict(X_test)\n\n# Mostrar algunas predicciones junto con los valores reales\npredictions_df = pd.DataFrame({'Valor Real': y_test, 'Predicci√≥n': y_pred})\nprint(\"--- Muestra de Predicciones vs. Valores Reales ---\")\nprint(predictions_df.head(10))\n\n\n--- Muestra de Predicciones vs. Valores Reales ---\n     Valor Real  Predicci√≥n\n124           4           4\n54            4           4\n67            4           4\n20            4           4\n36            3           5\n118           4           3\n136           3           4\n106           5           4\n25            4           4\n64            2           4"
  },
  {
    "objectID": "midterm_clasificacion.html#evaluaci√≥n-del-modelo",
    "href": "midterm_clasificacion.html#evaluaci√≥n-del-modelo",
    "title": "MidTerm Clasificaci√≥n",
    "section": "7. Evaluaci√≥n del Modelo",
    "text": "7. Evaluaci√≥n del Modelo\nPara medir qu√© tan bien funcion√≥ nuestro modelo, utilizamos m√©tricas de clasificaci√≥n clave:\n\nAccuracy (Exactitud): El porcentaje de predicciones correctas.\nClassification Report: Un resumen que incluye:\n\nPrecision: De todas las veces que el modelo predijo una clase, ¬øqu√© porcentaje fue correcto?\nRecall (Sensibilidad): De todos los ejemplos reales de una clase, ¬øqu√© porcentaje identific√≥ correctamente el modelo?\nF1-Score: La media arm√≥nica de precisi√≥n y recall, √∫til para clases desbalanceadas.\n\nMatriz de Confusi√≥n: Una tabla que visualiza el rendimiento, mostrando los verdaderos positivos, falsos positivos, verdaderos negativos y falsos negativos.\n\n\n\n\nCode\n# Calcular la exactitud del modelo\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"--- Exactitud (Accuracy) del Modelo ---\")\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# Generar el reporte de clasificaci√≥n\nprint(\"\\n--- Reporte de Clasificaci√≥n ---\")\nprint(classification_report(y_test, y_pred))\n\n# Generar la matriz de confusi√≥n\nprint(\"\\n--- Matriz de Confusi√≥n ---\")\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(conf_matrix)\n\n\n--- Exactitud (Accuracy) del Modelo ---\nAccuracy: 0.5357\n\n--- Reporte de Clasificaci√≥n ---\n              precision    recall  f1-score   support\n\n           1       1.00      1.00      1.00         1\n           2       0.00      0.00      0.00         2\n           3       0.33      0.14      0.20         7\n           4       0.50      0.82      0.62        11\n           5       0.67      0.57      0.62         7\n\n    accuracy                           0.54        28\n   macro avg       0.50      0.51      0.49        28\nweighted avg       0.48      0.54      0.48        28\n\n\n--- Matriz de Confusi√≥n ---\n[[1 0 0 0 0]\n [0 0 0 2 0]\n [0 0 1 5 1]\n [0 0 1 9 1]\n [0 0 1 2 4]]\n\n\nC:\\Personal\\Yachay\\machine_learning\\.venv-13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\nC:\\Personal\\Yachay\\machine_learning\\.venv-13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\nC:\\Personal\\Yachay\\machine_learning\\.venv-13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])"
  },
  {
    "objectID": "midterm_clasificacion.html#visualizaciones-e-interpretaci√≥n-de-resultados",
    "href": "midterm_clasificacion.html#visualizaciones-e-interpretaci√≥n-de-resultados",
    "title": "MidTerm Clasificaci√≥n",
    "section": "8. Visualizaciones e Interpretaci√≥n de Resultados",
    "text": "8. Visualizaciones e Interpretaci√≥n de Resultados\nUna visualizaci√≥n de la matriz de confusi√≥n facilita enormemente su interpretaci√≥n. Usaremos un mapa de calor (heatmap) de Seaborn.\n\n\nCode\n# Visualizar la matriz de confusi√≥n con un mapa de calor\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=np.unique(y), yticklabels=np.unique(y))\nplt.xlabel('Etiqueta Predicha')\nplt.ylabel('Etiqueta Real')\nplt.title('Matriz de Confusi√≥n')\nplt.show()\n\n\n\n\n\nMatriz de Confusi√≥n del Modelo de Regresi√≥n Log√≠stica\n\n\n\n\n\nInterpretaci√≥n Final üß†\n\nRendimiento General: La exactitud (accuracy) nos da una idea general del porcentaje de predicciones correctas. Sin embargo, en problemas con m√∫ltiples clases, no cuenta toda la historia.\nReporte de Clasificaci√≥n: Analizando el reporte, podemos ver el rendimiento del modelo para cada nivel de estr√©s. Por ejemplo, podr√≠amos notar que el modelo tiene alta precisi√≥n y recall para las clases con m√°s muestras (ej. estr√©s nivel 3 o 4), pero un rendimiento m√°s bajo para clases menos frecuentes.\nMatriz de Confusi√≥n: El gr√°fico nos permite ver exactamente d√≥nde se equivoca el modelo. La diagonal principal (de arriba a la izquierda a abajo a la derecha) muestra las predicciones correctas. Los n√∫meros fuera de la diagonal son los errores. Por ejemplo, un n√∫mero alto en la fila ‚ÄúReal 4‚Äù y la columna ‚ÄúPredicha 3‚Äù indicar√≠a que el modelo tiende a confundir un nivel de estr√©s real de 4 con uno de 3.\n\nEn resumen, el modelo de Regresi√≥n Log√≠stica implementado a trav√©s de un pipeline nos ofrece una base s√≥lida para predecir el estr√©s acad√©mico. Para mejorar los resultados, los siguientes pasos podr√≠an incluir la ingenier√≠a de caracter√≠sticas o probar modelos m√°s complejos, siempre manteniendo una estructura de trabajo ordenada como la que hemos definido aqu√≠."
  },
  {
    "objectID": "midterm_clasificacion.html#carga-el-dataset",
    "href": "midterm_clasificacion.html#carga-el-dataset",
    "title": "MidTerm Clasificaci√≥n",
    "section": "2. Carga el Dataset",
    "text": "2. Carga el Dataset\nPrimero, cargamos las librer√≠as necesarias y el conjunto de datos. Realizaremos una exploraci√≥n b√°sica para entender su estructura, identificar las variables num√©ricas y categ√≥ricas, y prepararlo para el modelo.\n\n\nCode\n# Cargar el dataset desde el archivo CSV\nfile_path = 'AcademicStressLevel.csv'\ndf = pd.read_csv(file_path)"
  },
  {
    "objectID": "midterm_clasificacion.html#exploraci√≥n-inicial-del-dataset",
    "href": "midterm_clasificacion.html#exploraci√≥n-inicial-del-dataset",
    "title": "MidTerm Clasificaci√≥n",
    "section": "3. Exploraci√≥n Inicial del Dataset",
    "text": "3. Exploraci√≥n Inicial del Dataset\n\n3.1 Exploraci√≥n inicial\n\n\nCode\n# --- Exploraci√≥n Inicial ---\nprint(\"--- Informaci√≥n General del Dataset ---\")\ndf.info()\n\nprint(\"\\n--- Primeras 5 Filas del Dataset ---\")\nprint(df.head())\n\n\n--- Informaci√≥n General del Dataset ---\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 140 entries, 0 to 139\nData columns (total 9 columns):\n #   Column                                                               Non-Null Count  Dtype \n---  ------                                                               --------------  ----- \n 0   Timestamp                                                            140 non-null    object\n 1   Your Academic Stage                                                  140 non-null    object\n 2   Peer pressure                                                        140 non-null    int64 \n 3   Academic pressure from your home                                     140 non-null    int64 \n 4   Study Environment                                                    139 non-null    object\n 5   What coping strategy you use as a student?                           140 non-null    object\n 6   Do you have any bad habits like smoking, drinking on a daily basis?  140 non-null    object\n 7   What would you rate the academic  competition in your student life   140 non-null    int64 \n 8   Rate your academic stress index                                      140 non-null    int64 \ndtypes: int64(4), object(5)\nmemory usage: 10.0+ KB\n\n--- Primeras 5 Filas del Dataset ---\n             Timestamp Your Academic Stage  Peer pressure  \\\n0  24/07/2025 22:05:39       undergraduate              4   \n1  24/07/2025 22:05:52       undergraduate              3   \n2  24/07/2025 22:06:39       undergraduate              1   \n3  24/07/2025 22:06:45       undergraduate              3   \n4  24/07/2025 22:08:06       undergraduate              3   \n\n   Academic pressure from your home Study Environment  \\\n0                                 5             Noisy   \n1                                 4          Peaceful   \n2                                 1          Peaceful   \n3                                 2          Peaceful   \n4                                 3          Peaceful   \n\n          What coping strategy you use as a student?  \\\n0  Analyze the situation and handle it with intel...   \n1  Analyze the situation and handle it with intel...   \n2                   Social support (friends, family)   \n3  Analyze the situation and handle it with intel...   \n4  Analyze the situation and handle it with intel...   \n\n  Do you have any bad habits like smoking, drinking on a daily basis?  \\\n0                                                 No                    \n1                                                 No                    \n2                                                 No                    \n3                                                 No                    \n4                                                 No                    \n\n   What would you rate the academic  competition in your student life  \\\n0                                                  3                    \n1                                                  3                    \n2                                                  2                    \n3                                                  4                    \n4                                                  4                    \n\n   Rate your academic stress index   \n0                                 5  \n1                                 3  \n2                                 4  \n3                                 3  \n4                                 5  \n\n\n\n\n3.2 renombrado de clumnas\nSe renombra kas columnas para mejor facilidad de manipularlas\n\n\nCode\n# --- Limpieza y Selecci√≥n de Caracter√≠sticas ---\n# Limpiar nombres de columnas antes de renombrar\ndf.columns = df.columns.str.strip()\n\n# Los nombres de las columnas son largos y contienen espacios. Vamos a renombrarlos.\ncolumn_mapping = {\n    'Your Academic Stage': 'academic_stage',\n    'Peer pressure': 'peer_pressure',\n    'Academic pressure from your home': 'home_pressure',\n    'Study Environment': 'study_environment',\n    'What coping strategy you use as a student?': 'coping_strategy',\n    'Do you have any bad habits like smoking, drinking on a daily basis?': 'bad_habits',\n    'What would you rate the academic  competition in your student life': 'competition_rating',\n    'Rate your academic stress index': 'stress_index'\n}\ndf = df.rename(columns=column_mapping)\n\n\nVer columnas renombradas\n\n\nCode\nprint(\"--- Informaci√≥n General del Dataset renombado ---\")\ndf.info()\n\n\n--- Informaci√≥n General del Dataset renombado ---\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 140 entries, 0 to 139\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   Timestamp           140 non-null    object\n 1   academic_stage      140 non-null    object\n 2   peer_pressure       140 non-null    int64 \n 3   home_pressure       140 non-null    int64 \n 4   study_environment   139 non-null    object\n 5   coping_strategy     140 non-null    object\n 6   bad_habits          140 non-null    object\n 7   competition_rating  140 non-null    int64 \n 8   stress_index        140 non-null    int64 \ndtypes: int64(4), object(5)\nmemory usage: 10.0+ KB\n\n\n\n\n3.3 Evitar el us de variables categ√≥ricas\n\n\nCode\n# Seg√∫n los lineamientos, debemos evitar el uso de variables categ√≥ricas.\n# Identificamos las columnas categ√≥ricas a eliminar:\ncategorical_cols = ['academic_stage', 'study_environment', 'coping_strategy', 'bad_habits']\n# Tambi√©n eliminamos 'Timestamp' por no ser relevante para el modelo.\ncols_to_drop = categorical_cols + ['Timestamp']\n\ndf_cleaned = df.drop(columns=cols_to_drop)\n\n# Verificamos si hay valores nulos\nprint(f\"\\n--- Valores Nulos por Columna (despu√©s de limpiar) ---\")\nprint(df_cleaned.isnull().sum())\n\n# Si hubiera valores nulos, una opci√≥n ser√≠a eliminarlos.\n# df_cleaned = df_cleaned.dropna()\n\nprint(\"\\n--- Dataset Limpio (solo variables num√©ricas) ---\")\nprint(df_cleaned.head())\n\nprint(\"\\n--- Descripci√≥n Estad√≠stica del Dataset Limpio ---\")\nprint(df_cleaned.describe())\n\n\n\n--- Valores Nulos por Columna (despu√©s de limpiar) ---\npeer_pressure         0\nhome_pressure         0\ncompetition_rating    0\nstress_index          0\ndtype: int64\n\n--- Dataset Limpio (solo variables num√©ricas) ---\n   peer_pressure  home_pressure  competition_rating  stress_index\n0              4              5                   3             5\n1              3              4                   3             3\n2              1              1                   2             4\n3              3              2                   4             3\n4              3              3                   4             5\n\n--- Descripci√≥n Estad√≠stica del Dataset Limpio ---\n       peer_pressure  home_pressure  competition_rating  stress_index\ncount     140.000000     140.000000          140.000000    140.000000\nmean        3.071429       3.178571            3.492857      3.721429\nstd         1.083844       1.276618            1.028349      1.032339\nmin         1.000000       1.000000            1.000000      1.000000\n25%         2.000000       2.000000            3.000000      3.000000\n50%         3.000000       3.000000            4.000000      4.000000\n75%         4.000000       4.000000            4.000000      4.000000\nmax         5.000000       5.000000            5.000000      5.000000\n\n\nInterpretaci√≥n de la Exploraci√≥n: El dataset original contiene una mezcla de variables num√©ricas y categ√≥ricas. Para cumplir con los requisitos, hemos eliminado las columnas categ√≥ricas (academic_stage, study_environment, etc.) y la columna Timestamp. Nos quedamos con las siguientes variables num√©ricas: peer_pressure, home_pressure, competition_rating como nuestras caracter√≠sticas (features), y stress_index como nuestra variable objetivo (target). Afortunadamente, no se encontraron valores nulos en las columnas seleccionadas."
  },
  {
    "objectID": "categorical_encoding.html#importar-librer√≠as",
    "href": "categorical_encoding.html#importar-librer√≠as",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Importar librer√≠as\")\n\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\nImportar librer√≠as"
  },
  {
    "objectID": "categorical_encoding.html#dataframe-de-ejemplo",
    "href": "categorical_encoding.html#dataframe-de-ejemplo",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Armar dataframe de ejemlpo\")\n\ndata = {\n    'size' : ['M', 'S', 'L'], # Categ√≥rico ordinal = OrdinalEncoder()\n    'color' : ['green', 'red', 'blue'], # categ√≥rico nominal = OneHotencoder()\n    'price' : [10.1, 13.5, 15.6], # num√©rico continuo: StandarScalar()\n    'label' : ['pantalones', 'camisetas', 'camisetas'] # etiqueta = LabelEncoder()\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\nArmar dataframe de ejemlpo\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n\n\n1\nS\nred\n13.5\ncamisetas\n\n\n2\nL\nblue\n15.6\ncamisetas"
  },
  {
    "objectID": "categorical_encoding.html#labelencoder-usar-solo-en-la-columna-target-output",
    "href": "categorical_encoding.html#labelencoder-usar-solo-en-la-columna-target-output",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Aplicar LabelEncoder\")\n\ndf['label_encoded'] = LabelEncoder().fit_transform(df['label'])\ndf\n\n\nAplicar LabelEncoder\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0"
  },
  {
    "objectID": "categorical_encoding.html#ordinalencoder-usar-en-dond-elas-columnas-tengan-un-orden-natural-e.g.-s-m-l",
    "href": "categorical_encoding.html#ordinalencoder-usar-en-dond-elas-columnas-tengan-un-orden-natural-e.g.-s-m-l",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Aplicar OrdinalEncoder\")\n\nsize_order = [['S',  'M', 'L']]\ndf['size_encoded'] = OrdinalEncoder(categories=size_order).fit_transform(df[['size']])\ndf\n\n\nAplicar OrdinalEncoder\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\nsize_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n1.0\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n0.0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n2.0"
  },
  {
    "objectID": "categorical_encoding.html#onehotencoder",
    "href": "categorical_encoding.html#onehotencoder",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Aplicar OneHotEncoder\")\n\ncolor_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\ncolor_encoded = color_encoder.fit_transform(df[['color']])\n\ncolor_title_encoded = color_encoder.get_feature_names_out(['color'])\n\ndf[color_title_encoded] = color_encoded\n\ndf\n\n\nAplicar OneHotEncoder\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\nsize_encoded\ncolor_blue\ncolor_green\ncolor_red\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n1.0\n0.0\n1.0\n0.0\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n0.0\n0.0\n0.0\n1.0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n2.0\n1.0\n0.0\n0.0"
  },
  {
    "objectID": "categorical_encoding.html#columntransformer",
    "href": "categorical_encoding.html#columntransformer",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Aplicar ColumnTransformer\")\n\ncategorical_features = ['color']\nordinal_features = ['size']\nnumerical_features = ['price']\n\nsize_order = [['S',  'M', 'L']]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('color_onehot', OneHotEncoder(),   categorical_features),\n        ('size_encoder', OrdinalEncoder(categories=size_order), ordinal_features),\n        ('price_scaler', StandardScaler(), numerical_features)\n    ]\n)\n\n\nAplicar ColumnTransformer"
  },
  {
    "objectID": "categorical_encoding.html#aplicar-el-preprocessor",
    "href": "categorical_encoding.html#aplicar-el-preprocessor",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Aplicar el preprocessor\")\n\nfeactures_transformed = preprocessor.fit_transform(df)\nfeactures_transformed\n\n\nAplicar el preprocessor\n\n\narray([[ 0.        ,  1.        ,  0.        ,  1.        , -1.30910667],\n       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.19121783],\n       [ 1.        ,  0.        ,  0.        ,  2.        ,  1.11788884]])"
  },
  {
    "objectID": "categorical_pieline.html",
    "href": "categorical_pieline.html",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Importar librer√≠as\")\n\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\n\n\nImportar librer√≠as\n\n\n\n\n\n\n\nCode\nprint(\"cargar dataset\")\n\ndata = fetch_openml('adult', version=2, as_frame=True)\n\ndf = data.frame.copy()\ndf\n\n\ncargar dataset\n\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nclass\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n&lt;=50K\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n&gt;50K\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48837\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n&lt;=50K\n\n\n48838\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n48839\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n48840\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n&lt;=50K\n\n\n48841\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n&gt;50K\n\n\n\n\n48842 rows √ó 15 columns\n\n\n\n\n\n\n\n\nCode\nprint(\"Preprocesamiento inicial\")\ndf.dropna()\n\n\nPreprocesamiento inicial\n\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nclass\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n&lt;=50K\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n&gt;50K\n\n\n5\n34\nPrivate\n198693\n10th\n6\nNever-married\nOther-service\nNot-in-family\nWhite\nMale\n0\n0\n30\nUnited-States\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48837\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n&lt;=50K\n\n\n48838\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n48839\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n48840\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n&lt;=50K\n\n\n48841\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n&gt;50K\n\n\n\n\n45222 rows √ó 15 columns\n\n\n\n\n\n\n\n\nCode\nprint(\"Separaci√≥n de features y target\")\n\nX = df.drop(\"class\",axis=1)\ny = df['class']\n\n\nSeparaci√≥n de features y target\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
  },
  {
    "objectID": "categorical_pieline.html#importar-librer√≠as",
    "href": "categorical_pieline.html#importar-librer√≠as",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Importar librer√≠as\")\n\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\n\n\nImportar librer√≠as"
  },
  {
    "objectID": "categorical_pieline.html#cargar-dataset",
    "href": "categorical_pieline.html#cargar-dataset",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"cargar dataset\")\n\ndata = fetch_openml('adult', version=2, as_frame=True)\n\ndf = data.frame.copy()\ndf\n\n\ncargar dataset\n\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nclass\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n&lt;=50K\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n&gt;50K\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48837\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n&lt;=50K\n\n\n48838\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n48839\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n48840\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n&lt;=50K\n\n\n48841\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n&gt;50K\n\n\n\n\n48842 rows √ó 15 columns"
  },
  {
    "objectID": "categorical_encoding.html",
    "href": "categorical_encoding.html",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Importar librer√≠as\")\n\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\nImportar librer√≠as\n\n\n\n\n\n\n\nCode\nprint(\"Armar dataframe de ejemlpo\")\n\ndata = {\n    'size' : ['M', 'S', 'L'], # Categ√≥rico ordinal = OrdinalEncoder()\n    'color' : ['green', 'red', 'blue'], # categ√≥rico nominal = OneHotencoder()\n    'price' : [10.1, 13.5, 15.6], # num√©rico continuo: StandarScalar()\n    'label' : ['pantalones', 'camisetas', 'camisetas'] # etiqueta = LabelEncoder()\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\nArmar dataframe de ejemlpo\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n\n\n1\nS\nred\n13.5\ncamisetas\n\n\n2\nL\nblue\n15.6\ncamisetas\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"Aplicar LabelEncoder\")\n\ndf['label_encoded'] = LabelEncoder().fit_transform(df['label'])\ndf\n\n\nAplicar LabelEncoder\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"Aplicar OrdinalEncoder\")\n\nsize_order = [['S',  'M', 'L']]\ndf['size_encoded'] = OrdinalEncoder(categories=size_order).fit_transform(df[['size']])\ndf\n\n\nAplicar OrdinalEncoder\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\nsize_encoded\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n1.0\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n0.0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n2.0\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"Aplicar OneHotEncoder\")\n\ncolor_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\ncolor_encoded = color_encoder.fit_transform(df[['color']])\n\ncolor_title_encoded = color_encoder.get_feature_names_out(['color'])\n\ndf[color_title_encoded] = color_encoded\n\ndf\n\n\nAplicar OneHotEncoder\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\nlabel_encoded\nsize_encoded\ncolor_blue\ncolor_green\ncolor_red\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n1\n1.0\n0.0\n1.0\n0.0\n\n\n1\nS\nred\n13.5\ncamisetas\n0\n0.0\n0.0\n0.0\n1.0\n\n\n2\nL\nblue\n15.6\ncamisetas\n0\n2.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"Aplicar ColumnTransformer\")\n\ncategorical_features = ['color']\nordinal_features = ['size']\nnumerical_features = ['price']\n\nsize_order = [['S',  'M', 'L']]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('color_onehot', OneHotEncoder(),   categorical_features),\n        ('size_encoder', OrdinalEncoder(categories=size_order), ordinal_features),\n        ('price_scaler', StandardScaler(), numerical_features)\n    ]\n)\n\n\nAplicar ColumnTransformer\n\n\n\n\n\n\n\nCode\nprint(\"Aplicar el preprocessor\")\n\nfeactures_transformed = preprocessor.fit_transform(df)\nfeactures_transformed\n\n\nAplicar el preprocessor\n\n\narray([[ 0.        ,  1.        ,  0.        ,  1.        , -1.30910667],\n       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.19121783],\n       [ 1.        ,  0.        ,  0.        ,  2.        ,  1.11788884]])"
  },
  {
    "objectID": "categorical_pieline.html#preprocesamiento",
    "href": "categorical_pieline.html#preprocesamiento",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Preprocesamiento\")\n\n\nPreprocesamiento"
  },
  {
    "objectID": "categorical_pieline.html#preprocesamiento-inicial",
    "href": "categorical_pieline.html#preprocesamiento-inicial",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Preprocesamiento inicial\")\ndf.dropna()\n\n\nPreprocesamiento inicial\n\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nclass\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n&lt;=50K\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n&gt;50K\n\n\n5\n34\nPrivate\n198693\n10th\n6\nNever-married\nOther-service\nNot-in-family\nWhite\nMale\n0\n0\n30\nUnited-States\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48837\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n&lt;=50K\n\n\n48838\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n48839\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n48840\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n&lt;=50K\n\n\n48841\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n&gt;50K\n\n\n\n\n45222 rows √ó 15 columns"
  },
  {
    "objectID": "categorical_pieline.html#separaci√≥n-de-features-y-target",
    "href": "categorical_pieline.html#separaci√≥n-de-features-y-target",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nprint(\"Separaci√≥n de features y target\")\n\nX = df.drop(\"class\",axis=1)\ny = df['class']\n\n\nSeparaci√≥n de features y target"
  },
  {
    "objectID": "categorical_pieline.html#separaci√≥n",
    "href": "categorical_pieline.html#separaci√≥n",
    "title": "Categorical encoding",
    "section": "",
    "text": "Code\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
  }
]